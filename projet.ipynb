{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RABHI Alexadre\n",
    "* ZANIN Julien\n",
    "\n",
    "GitHub : \"https://github.com/Julien-Zanin/ML-DataChallenge.git\"\n",
    "\n",
    "**Insérer une table des matières**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Sujet : [Challenge Collège de France : prédiction de rendement](https://challengedata.ens.fr/participants/challenges/84/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Contexte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prédiction du mouvement de la direction d'un rendement d'un actif sur la période de 14h-16h en partant des rendements de 9h à 14H, calculés à partir d'un pas de temps de 5 minutes. \n",
    "\n",
    "Chaques actif est propre à un dateset, les actifs du jeu d'entraînement sont différents des actifs du jeu de test. \n",
    "\n",
    "Chaque ligne est identifiable par un ID unique qui correspond à un couple jour - equity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'input_training.csv'\n",
    "X = pd.read_csv(path)\n",
    "X.sort_values(by=\"ID\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r43</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>1000384</td>\n",
       "      <td>1000064</td>\n",
       "      <td>79.19</td>\n",
       "      <td>-26.37</td>\n",
       "      <td>-167.18</td>\n",
       "      <td>103.46</td>\n",
       "      <td>-102.27</td>\n",
       "      <td>-198.02</td>\n",
       "      <td>13.77</td>\n",
       "      <td>...</td>\n",
       "      <td>-116.69</td>\n",
       "      <td>29.82</td>\n",
       "      <td>-29.69</td>\n",
       "      <td>77.57</td>\n",
       "      <td>23.75</td>\n",
       "      <td>82.94</td>\n",
       "      <td>-17.63</td>\n",
       "      <td>17.60</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000001</td>\n",
       "      <td>1000148</td>\n",
       "      <td>1000841</td>\n",
       "      <td>-321.77</td>\n",
       "      <td>-178.53</td>\n",
       "      <td>107.97</td>\n",
       "      <td>27.29</td>\n",
       "      <td>-64.54</td>\n",
       "      <td>-73.78</td>\n",
       "      <td>9.93</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>1.53</td>\n",
       "      <td>32.08</td>\n",
       "      <td>-33.50</td>\n",
       "      <td>-21.41</td>\n",
       "      <td>43.89</td>\n",
       "      <td>68.07</td>\n",
       "      <td>-23.20</td>\n",
       "      <td>-13.14</td>\n",
       "      <td>-82.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000441</td>\n",
       "      <td>1000380</td>\n",
       "      <td>-51.95</td>\n",
       "      <td>-19.58</td>\n",
       "      <td>-26.16</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>3.29</td>\n",
       "      <td>-46.11</td>\n",
       "      <td>...</td>\n",
       "      <td>13.56</td>\n",
       "      <td>-10.16</td>\n",
       "      <td>-40.68</td>\n",
       "      <td>13.61</td>\n",
       "      <td>3.40</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-6.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000003</td>\n",
       "      <td>1000059</td>\n",
       "      <td>1001843</td>\n",
       "      <td>-169.49</td>\n",
       "      <td>9.57</td>\n",
       "      <td>-76.48</td>\n",
       "      <td>28.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.21</td>\n",
       "      <td>105.87</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.72</td>\n",
       "      <td>-9.72</td>\n",
       "      <td>-9.73</td>\n",
       "      <td>-9.74</td>\n",
       "      <td>29.24</td>\n",
       "      <td>-29.18</td>\n",
       "      <td>34.15</td>\n",
       "      <td>14.58</td>\n",
       "      <td>-29.13</td>\n",
       "      <td>4.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000004</td>\n",
       "      <td>1000336</td>\n",
       "      <td>1001783</td>\n",
       "      <td>-46.19</td>\n",
       "      <td>32.17</td>\n",
       "      <td>-3.08</td>\n",
       "      <td>-32.06</td>\n",
       "      <td>11.75</td>\n",
       "      <td>27.81</td>\n",
       "      <td>-13.55</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.68</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>1.23</td>\n",
       "      <td>5.53</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885794</th>\n",
       "      <td>1885794</td>\n",
       "      <td>1000125</td>\n",
       "      <td>1001131</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.87</td>\n",
       "      <td>-67.41</td>\n",
       "      <td>-18.15</td>\n",
       "      <td>12.99</td>\n",
       "      <td>-42.80</td>\n",
       "      <td>29.96</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.27</td>\n",
       "      <td>5.09</td>\n",
       "      <td>-12.73</td>\n",
       "      <td>5.10</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>-20.41</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7.67</td>\n",
       "      <td>6.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885795</th>\n",
       "      <td>1885795</td>\n",
       "      <td>1000314</td>\n",
       "      <td>1000333</td>\n",
       "      <td>76.29</td>\n",
       "      <td>-65.02</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-24.54</td>\n",
       "      <td>-17.78</td>\n",
       "      <td>13.70</td>\n",
       "      <td>-19.17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-13.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.93</td>\n",
       "      <td>8.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885796</th>\n",
       "      <td>1885796</td>\n",
       "      <td>1000247</td>\n",
       "      <td>1000348</td>\n",
       "      <td>-73.66</td>\n",
       "      <td>37.11</td>\n",
       "      <td>-7.92</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-18.49</td>\n",
       "      <td>-26.46</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.97</td>\n",
       "      <td>10.63</td>\n",
       "      <td>5.31</td>\n",
       "      <td>-10.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.28</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>6.64</td>\n",
       "      <td>7.96</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885797</th>\n",
       "      <td>1885797</td>\n",
       "      <td>1000035</td>\n",
       "      <td>1000040</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-102.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885798</th>\n",
       "      <td>1885798</td>\n",
       "      <td>1000452</td>\n",
       "      <td>1000260</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-6.81</td>\n",
       "      <td>-5.45</td>\n",
       "      <td>4.09</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-4.08</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>5.44</td>\n",
       "      <td>9.51</td>\n",
       "      <td>-5.43</td>\n",
       "      <td>-4.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>885799 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID      day   equity      r0      r1      r2      r3      r4  \\\n",
       "0       1000000  1000384  1000064   79.19  -26.37 -167.18  103.46 -102.27   \n",
       "1       1000001  1000148  1000841 -321.77 -178.53  107.97   27.29  -64.54   \n",
       "2       1000002  1000441  1000380  -51.95  -19.58  -26.16  -26.28   -3.29   \n",
       "3       1000003  1000059  1001843 -169.49    9.57  -76.48   28.90    0.00   \n",
       "4       1000004  1000336  1001783  -46.19   32.17   -3.08  -32.06   11.75   \n",
       "...         ...      ...      ...     ...     ...     ...     ...     ...   \n",
       "885794  1885794  1000125  1001131    0.00   67.87  -67.41  -18.15   12.99   \n",
       "885795  1885795  1000314  1000333   76.29  -65.02    2.73  -24.54  -17.78   \n",
       "885796  1885796  1000247  1000348  -73.66   37.11   -7.92   -7.93    0.00   \n",
       "885797  1885797  1000035  1000040    0.00 -102.74     NaN     NaN     NaN   \n",
       "885798  1885798  1000452  1000260    6.81    1.36   -6.81   -5.45    4.09   \n",
       "\n",
       "            r5      r6  ...     r43    r44    r45    r46    r47    r48    r49  \\\n",
       "0      -198.02   13.77  ... -116.69  29.82 -29.69  77.57  23.75  82.94 -17.63   \n",
       "1       -73.78    9.93  ...  -12.21   1.53  32.08 -33.50 -21.41  43.89  68.07   \n",
       "2         3.29  -46.11  ...   13.56 -10.16 -40.68  13.61   3.40   6.80  10.20   \n",
       "3       -19.21  105.87  ...   -9.72  -9.72  -9.73  -9.74  29.24 -29.18  34.15   \n",
       "4        27.81  -13.55  ...   -3.68  -1.84  -1.23   1.23   5.53  -7.37   1.84   \n",
       "...        ...     ...  ...     ...    ...    ...    ...    ...    ...    ...   \n",
       "885794  -42.80   29.96  ...   -1.27   5.09 -12.73   5.10  -8.91 -20.41   2.56   \n",
       "885795   13.70  -19.17  ...    1.36  -1.36 -13.65   0.00   0.00  -4.10  -2.73   \n",
       "885796  -18.49  -26.46  ...   -7.97  10.63   5.31 -10.61   0.00  13.28  -5.31   \n",
       "885797     NaN     NaN  ...    0.00    NaN   0.00    NaN   0.00   0.00    NaN   \n",
       "885798    8.17    0.00  ...    1.36  -4.08  -2.72   1.36   2.72  -1.36   5.44   \n",
       "\n",
       "          r50    r51    r52  \n",
       "0       17.60   5.87   0.00  \n",
       "1      -23.20 -13.14 -82.95  \n",
       "2        0.00 -10.19  -6.80  \n",
       "3       14.58 -29.13   4.87  \n",
       "4        1.23   0.61  -4.30  \n",
       "...       ...    ...    ...  \n",
       "885794   0.00  -7.67   6.40  \n",
       "885795  10.94  10.93   8.18  \n",
       "885796   6.64   7.96   5.30  \n",
       "885797   0.00    NaN    NaN  \n",
       "885798   9.51  -5.43  -4.07  \n",
       "\n",
       "[885799 rows x 56 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv(r\"input_test.csv\")\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843294</th>\n",
       "      <td>843294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843295</th>\n",
       "      <td>843295</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843296</th>\n",
       "      <td>843296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843297</th>\n",
       "      <td>843297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843298</th>\n",
       "      <td>843298</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  reod\n",
       "0            0     0\n",
       "1            1     0\n",
       "2            2    -1\n",
       "3            3     0\n",
       "4            4    -1\n",
       "...        ...   ...\n",
       "843294  843294     1\n",
       "843295  843295    -1\n",
       "843296  843296     0\n",
       "843297  843297     1\n",
       "843298  843298    -1\n",
       "\n",
       "[843299 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'output\\output_training_gmEd6Zt.csv'\n",
    "y = pd.read_csv(path)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On va analyser le nombre de NaN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_rendements = [col for col in X.columns if col.startswith(\"r\")]\n",
    "\n",
    "NaN_analysis = pd.DataFrame()\n",
    "NaN_analysis[\"NaN_count\"] = X[col_rendements].isna().sum(axis=1)\n",
    "\n",
    "nombre_colonnes = len(col_rendements)\n",
    "NaN_analysis[\"NaN_percent\"] = (NaN_analysis[\"NaN_count\"]/nombre_colonnes)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne totale du dataset 843299\n",
      "nombre de lignes contenant au moins des NaN 242363, soit en %: 28.739865694136956\n",
      "nombre de lignes avec plus 30% de NaN dans une colonne 80786, soit en % 14.89898600614966\n"
     ]
    }
   ],
   "source": [
    "nbr_row_na = X.isna().any(axis=1).sum()\n",
    "print(f\"Nombre de ligne totale du dataset {len(X)}\")\n",
    "print(f\"nombre de lignes contenant au moins des NaN {nbr_row_na}, soit en %: {(nbr_row_na/len(X)*100)}\")\n",
    "print(f\"nombre de lignes avec plus 30% de NaN dans une colonne {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>50])}, soit en % {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>30])/len(X)*100}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_random_bench = pd.read_csv(r\"output\\output_test_random.csv\")\n",
    "input_test = pd.read_csv(r\"input_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de lignes data input train 843299\n",
      "Nombre de lignes output_training 843299\n",
      "Nombre de lignes data input test 885799\n",
      "Nombre de lignes de l'output random bench 885799\n"
     ]
    }
   ],
   "source": [
    "print(f\"nombre de lignes data input train {len(X)}\")\n",
    "print(f\"Nombre de lignes output_training {len(y)}\")\n",
    "print(f\"Nombre de lignes data input test {len(input_test)}\")\n",
    "print(f\"Nombre de lignes de l'output random bench {len(output_random_bench)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# insert your code here\n",
    "plt.figure(figsize=(16,4))\n",
    "(X.isna().sum()).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On commence par fusionner les données de base sur la base de l'identifiant\n",
    "df_train = pd.merge(X,y,on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843294</th>\n",
       "      <td>843294</td>\n",
       "      <td>297</td>\n",
       "      <td>123</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-70.34</td>\n",
       "      <td>74.24</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-21.62</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843295</th>\n",
       "      <td>843295</td>\n",
       "      <td>16</td>\n",
       "      <td>1501</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-183.49</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>46.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.60</td>\n",
       "      <td>13.25</td>\n",
       "      <td>...</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.42</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.88</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843296</th>\n",
       "      <td>843296</td>\n",
       "      <td>166</td>\n",
       "      <td>1231</td>\n",
       "      <td>37.02</td>\n",
       "      <td>2.93</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>16.89</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>13.56</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-13.51</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>9.69</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843297</th>\n",
       "      <td>843297</td>\n",
       "      <td>297</td>\n",
       "      <td>747</td>\n",
       "      <td>34.45</td>\n",
       "      <td>15.10</td>\n",
       "      <td>-35.61</td>\n",
       "      <td>19.25</td>\n",
       "      <td>-16.46</td>\n",
       "      <td>-26.12</td>\n",
       "      <td>20.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.52</td>\n",
       "      <td>-6.90</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>33.16</td>\n",
       "      <td>13.77</td>\n",
       "      <td>12.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843298</th>\n",
       "      <td>843298</td>\n",
       "      <td>493</td>\n",
       "      <td>920</td>\n",
       "      <td>-25.91</td>\n",
       "      <td>-43.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>-17.48</td>\n",
       "      <td>-13.13</td>\n",
       "      <td>-13.15</td>\n",
       "      <td>30.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  day  equity      r0      r1     r2     r3     r4     r5  \\\n",
       "0            0  249    1488    0.00     NaN    NaN    NaN   0.00    NaN   \n",
       "1            1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00   \n",
       "2            2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14   \n",
       "3            3  302     513    0.00     NaN   0.00   0.00   0.00    NaN   \n",
       "4            4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   \n",
       "...        ...  ...     ...     ...     ...    ...    ...    ...    ...   \n",
       "843294  843294  297     123    3.96    0.00 -70.34  74.24  -0.56   0.00   \n",
       "843295  843295   16    1501    0.00 -183.49 -13.19  46.24   0.00 -39.60   \n",
       "843296  843296  166    1231   37.02    2.93  -3.67  16.89  -4.03  13.56   \n",
       "843297  843297  297     747   34.45   15.10 -35.61  19.25 -16.46 -26.12   \n",
       "843298  843298  493     920  -25.91  -43.37   0.00  17.42   0.00   0.00   \n",
       "\n",
       "           r6  ...    r44    r45    r46    r47    r48    r49    r50    r51  \\\n",
       "0         NaN  ...   0.00    NaN   0.00    NaN   0.00    NaN    NaN    NaN   \n",
       "1       41.24  ... -16.92  -4.84   4.84   0.00   7.26  -9.68 -19.38   9.71   \n",
       "2       49.12  ...   1.59   6.37 -49.32  -9.59  -6.40  22.41  -6.39   7.99   \n",
       "3         NaN  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN   0.00   \n",
       "4        0.00  ... -21.44 -21.48  10.78 -21.55  -5.40 -10.81   5.41 -32.47   \n",
       "...       ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "843294 -23.63  ...   1.71   0.00  -3.98   2.28 -21.62  -1.71   9.12   0.00   \n",
       "843295  13.25  ...   6.62   0.00  19.85   0.00 -26.42   6.62   0.00   0.00   \n",
       "843296  -4.39  ...  -3.28  -1.46  -3.65  -1.10 -13.51   2.92  -6.21   9.69   \n",
       "843297  20.68  ...  -5.52  -6.90   9.67   1.38   6.90 -11.04  33.16  13.77   \n",
       "843298  21.74  ...   8.75 -17.48 -13.13 -13.15  30.74   0.00   0.00  -4.39   \n",
       "\n",
       "          r52  reod  \n",
       "0        0.00     0  \n",
       "1       26.68     0  \n",
       "2       15.96    -1  \n",
       "3         NaN     0  \n",
       "4       43.43    -1  \n",
       "...       ...   ...  \n",
       "843294   9.11     1  \n",
       "843295 -19.88    -1  \n",
       "843296  -3.66     0  \n",
       "843297  12.38     1  \n",
       "843298  -8.79    -1  \n",
       "\n",
       "[843299 rows x 57 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin d'améliorer le modèle, on pourrait rajouter de nouvelles features : moyenne des rendements matinaux, volatilité...\n",
    "\n",
    "On peut réflechir à l'encodage de certaines variables. Day pourrait être pertinent si les dates sont communes entre le data set de train et test en admettant qu'ils partent de la même base pour les jours. Nous n'avons pas de date précise. Par contre on peut essayer d'observer s'il y'a un mouvemenbt global haussier, baissier ou si par exemple certains jours, même si toutes les actions sont différentes, vont avoir le même mouvement après un mouvement prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equity\n",
      "549     503\n",
      "1309    503\n",
      "871     503\n",
      "1146    503\n",
      "475     503\n",
      "       ... \n",
      "57       15\n",
      "1064     12\n",
      "742      11\n",
      "595       6\n",
      "659       3\n",
      "Name: count, Length: 1829, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X[\"equity\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equity\n",
      "549     503\n",
      "1309    503\n",
      "871     503\n",
      "1146    503\n",
      "475     503\n",
      "       ... \n",
      "57       15\n",
      "1064     12\n",
      "742      11\n",
      "595       6\n",
      "659       3\n",
      "Name: count, Length: 1829, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X[\"equity\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Premiers modèles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratégie simple (seulement sur les rendements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "r0     0\n",
      "r1     0\n",
      "r2     0\n",
      "r3     0\n",
      "r4     0\n",
      "r5     0\n",
      "r6     0\n",
      "r7     0\n",
      "r8     0\n",
      "r9     0\n",
      "r10    0\n",
      "r11    0\n",
      "r12    0\n",
      "r13    0\n",
      "r14    0\n",
      "r15    0\n",
      "r16    0\n",
      "r17    0\n",
      "r18    0\n",
      "r19    0\n",
      "r20    0\n",
      "r21    0\n",
      "r22    0\n",
      "r23    0\n",
      "r24    0\n",
      "r25    0\n",
      "r26    0\n",
      "r27    0\n",
      "r28    0\n",
      "r29    0\n",
      "r30    0\n",
      "r31    0\n",
      "r32    0\n",
      "r33    0\n",
      "r34    0\n",
      "r35    0\n",
      "r36    0\n",
      "r37    0\n",
      "r38    0\n",
      "r39    0\n",
      "r40    0\n",
      "r41    0\n",
      "r42    0\n",
      "r43    0\n",
      "r44    0\n",
      "r45    0\n",
      "r46    0\n",
      "r47    0\n",
      "r48    0\n",
      "r49    0\n",
      "r50    0\n",
      "r51    0\n",
      "r52    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>r7</th>\n",
       "      <th>r8</th>\n",
       "      <th>r9</th>\n",
       "      <th>...</th>\n",
       "      <th>r43</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-68.03</td>\n",
       "      <td>-34.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>12.08</td>\n",
       "      <td>-26.54</td>\n",
       "      <td>19.32</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>53.61</td>\n",
       "      <td>-4.70</td>\n",
       "      <td>-28.27</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-47.57</td>\n",
       "      <td>21.28</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.36</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843294</th>\n",
       "      <td>3.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-70.34</td>\n",
       "      <td>74.24</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23.63</td>\n",
       "      <td>-9.57</td>\n",
       "      <td>-5.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.80</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-21.62</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843295</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-183.49</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>46.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.60</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.42</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843296</th>\n",
       "      <td>37.02</td>\n",
       "      <td>2.93</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>16.89</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>13.56</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-14.28</td>\n",
       "      <td>-0.73</td>\n",
       "      <td>9.53</td>\n",
       "      <td>...</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-13.51</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>9.69</td>\n",
       "      <td>-3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843297</th>\n",
       "      <td>34.45</td>\n",
       "      <td>15.10</td>\n",
       "      <td>-35.61</td>\n",
       "      <td>19.25</td>\n",
       "      <td>-16.46</td>\n",
       "      <td>-26.12</td>\n",
       "      <td>20.68</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>2.75</td>\n",
       "      <td>-11.01</td>\n",
       "      <td>...</td>\n",
       "      <td>-20.65</td>\n",
       "      <td>-5.52</td>\n",
       "      <td>-6.90</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>33.16</td>\n",
       "      <td>13.77</td>\n",
       "      <td>12.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843298</th>\n",
       "      <td>-25.91</td>\n",
       "      <td>-43.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>-21.69</td>\n",
       "      <td>-26.10</td>\n",
       "      <td>-8.72</td>\n",
       "      <td>...</td>\n",
       "      <td>43.94</td>\n",
       "      <td>8.75</td>\n",
       "      <td>-17.48</td>\n",
       "      <td>-13.13</td>\n",
       "      <td>-13.15</td>\n",
       "      <td>30.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-8.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            r0      r1     r2     r3     r4     r5     r6     r7     r8  \\\n",
       "0         0.00    0.00   0.00   0.00   0.00   0.00   0.00 -68.03 -34.25   \n",
       "1        -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  12.08 -26.54   \n",
       "2        49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  53.61  -4.70   \n",
       "3         0.00    0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "4      -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00 -47.57  21.28   \n",
       "...        ...     ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "843294    3.96    0.00 -70.34  74.24  -0.56   0.00 -23.63  -9.57  -5.07   \n",
       "843295    0.00 -183.49 -13.19  46.24   0.00 -39.60  13.25   0.00   0.00   \n",
       "843296   37.02    2.93  -3.67  16.89  -4.03  13.56  -4.39 -14.28  -0.73   \n",
       "843297   34.45   15.10 -35.61  19.25 -16.46 -26.12  20.68  -2.75   2.75   \n",
       "843298  -25.91  -43.37   0.00  17.42   0.00   0.00  21.74 -21.69 -26.10   \n",
       "\n",
       "           r9  ...    r43    r44    r45    r46    r47    r48    r49    r50  \\\n",
       "0        0.00  ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1       19.32  ...  -4.83 -16.92  -4.84   4.84   0.00   7.26  -9.68 -19.38   \n",
       "2      -28.27  ...  -6.37   1.59   6.37 -49.32  -9.59  -6.40  22.41  -6.39   \n",
       "3        0.00  ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "4      -10.63  ...  -5.36 -21.44 -21.48  10.78 -21.55  -5.40 -10.81   5.41   \n",
       "...       ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "843294   0.00  ... -10.80   1.71   0.00  -3.98   2.28 -21.62  -1.71   9.12   \n",
       "843295  19.89  ...   0.00   6.62   0.00  19.85   0.00 -26.42   6.62   0.00   \n",
       "843296   9.53  ...   2.92  -3.28  -1.46  -3.65  -1.10 -13.51   2.92  -6.21   \n",
       "843297 -11.01  ... -20.65  -5.52  -6.90   9.67   1.38   6.90 -11.04  33.16   \n",
       "843298  -8.72  ...  43.94   8.75 -17.48 -13.13 -13.15  30.74   0.00   0.00   \n",
       "\n",
       "          r51    r52  \n",
       "0        0.00   0.00  \n",
       "1        9.71  26.68  \n",
       "2        7.99  15.96  \n",
       "3        0.00   0.00  \n",
       "4      -32.47  43.43  \n",
       "...       ...    ...  \n",
       "843294   0.00   9.11  \n",
       "843295   0.00 -19.88  \n",
       "843296   9.69  -3.66  \n",
       "843297  13.77  12.38  \n",
       "843298  -4.39  -8.79  \n",
       "\n",
       "[843299 rows x 53 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remplace les NA d'une ligne par la moyenne de sa colonne --> Peut fortement biaiser, c'est un simple test\n",
    "\n",
    "X_rendements = pd.DataFrame(X.loc[:,\"r0\":\"r52\"].fillna(0))\n",
    "print(X_rendements[\"r0\"].dtype)\n",
    "print(X_rendements.isna().sum())\n",
    "X_rendements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rendements,y,test_size=0.1, random_state=42)\n",
    "\n",
    "#On crée un pipeline :\n",
    "pipeline_bench = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=50,max_depth=10,random_state=42, n_jobs=1)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "could not allocate 12434948096 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpipeline_bench\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline_bench\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_val,y_pred))\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:660\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    655\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    656\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    657\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    658\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    659\u001b[0m         )\n\u001b[1;32m--> 660\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    476\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    479\u001b[0m ]\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    198\u001b[0m         X,\n\u001b[0;32m    199\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    203\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m_tree.pyx:153\u001b[0m, in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_tree.pyx:268\u001b[0m, in \u001b[0;36msklearn.tree._tree.DepthFirstTreeBuilder.build\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_tree.pyx:923\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree._add_node\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_tree.pyx:892\u001b[0m, in \u001b[0;36msklearn.tree._tree.Tree._resize_c\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_utils.pyx:29\u001b[0m, in \u001b[0;36msklearn.tree._utils.safe_realloc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: could not allocate 12434948096 bytes"
     ]
    }
   ],
   "source": [
    "pipeline_bench.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline_bench.predict(X_val)\n",
    "\n",
    "print(\"Accuracy\", accuracy_score(y_val,y_pred))\n",
    "print(\"Matrice de confusion : \\n\", confusion_matrix(y_val, y_pred))\n",
    "print(\"Rapport de classification: \\n\", classification_report(y_val,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
