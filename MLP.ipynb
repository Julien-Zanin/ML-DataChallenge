{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>r7</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-68.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>12.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>53.61</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>302</td>\n",
       "      <td>513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-47.57</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843294</th>\n",
       "      <td>297</td>\n",
       "      <td>123</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-70.34</td>\n",
       "      <td>74.24</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23.63</td>\n",
       "      <td>-9.57</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-21.62</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843295</th>\n",
       "      <td>16</td>\n",
       "      <td>1501</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-183.49</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>46.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.60</td>\n",
       "      <td>13.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.42</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.88</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843296</th>\n",
       "      <td>166</td>\n",
       "      <td>1231</td>\n",
       "      <td>37.02</td>\n",
       "      <td>2.93</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>16.89</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>13.56</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-14.28</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-13.51</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>9.69</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843297</th>\n",
       "      <td>297</td>\n",
       "      <td>747</td>\n",
       "      <td>34.45</td>\n",
       "      <td>15.10</td>\n",
       "      <td>-35.61</td>\n",
       "      <td>19.25</td>\n",
       "      <td>-16.46</td>\n",
       "      <td>-26.12</td>\n",
       "      <td>20.68</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.52</td>\n",
       "      <td>-6.90</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>33.16</td>\n",
       "      <td>13.77</td>\n",
       "      <td>12.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843298</th>\n",
       "      <td>493</td>\n",
       "      <td>920</td>\n",
       "      <td>-25.91</td>\n",
       "      <td>-43.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>-21.69</td>\n",
       "      <td>...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>-17.48</td>\n",
       "      <td>-13.13</td>\n",
       "      <td>-13.15</td>\n",
       "      <td>30.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        day  equity      r0      r1     r2     r3     r4     r5     r6     r7  \\\n",
       "ID                                                                              \n",
       "0       249    1488    0.00    0.00   0.00   0.00   0.00   0.00   0.00 -68.03   \n",
       "1       272     107   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  12.08   \n",
       "2       323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  53.61   \n",
       "3       302     513    0.00    0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "4       123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00 -47.57   \n",
       "...     ...     ...     ...     ...    ...    ...    ...    ...    ...    ...   \n",
       "843294  297     123    3.96    0.00 -70.34  74.24  -0.56   0.00 -23.63  -9.57   \n",
       "843295   16    1501    0.00 -183.49 -13.19  46.24   0.00 -39.60  13.25   0.00   \n",
       "843296  166    1231   37.02    2.93  -3.67  16.89  -4.03  13.56  -4.39 -14.28   \n",
       "843297  297     747   34.45   15.10 -35.61  19.25 -16.46 -26.12  20.68  -2.75   \n",
       "843298  493     920  -25.91  -43.37   0.00  17.42   0.00   0.00  21.74 -21.69   \n",
       "\n",
       "        ...    r44    r45    r46    r47    r48    r49    r50    r51    r52  \\\n",
       "ID      ...                                                                  \n",
       "0       ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1       ... -16.92  -4.84   4.84   0.00   7.26  -9.68 -19.38   9.71  26.68   \n",
       "2       ...   1.59   6.37 -49.32  -9.59  -6.40  22.41  -6.39   7.99  15.96   \n",
       "3       ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "4       ... -21.44 -21.48  10.78 -21.55  -5.40 -10.81   5.41 -32.47  43.43   \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "843294  ...   1.71   0.00  -3.98   2.28 -21.62  -1.71   9.12   0.00   9.11   \n",
       "843295  ...   6.62   0.00  19.85   0.00 -26.42   6.62   0.00   0.00 -19.88   \n",
       "843296  ...  -3.28  -1.46  -3.65  -1.10 -13.51   2.92  -6.21   9.69  -3.66   \n",
       "843297  ...  -5.52  -6.90   9.67   1.38   6.90 -11.04  33.16  13.77  12.38   \n",
       "843298  ...   8.75 -17.48 -13.13 -13.15  30.74   0.00   0.00  -4.39  -8.79   \n",
       "\n",
       "        reod  \n",
       "ID            \n",
       "0          0  \n",
       "1          0  \n",
       "2         -1  \n",
       "3          0  \n",
       "4         -1  \n",
       "...      ...  \n",
       "843294     1  \n",
       "843295    -1  \n",
       "843296     0  \n",
       "843297     1  \n",
       "843298    -1  \n",
       "\n",
       "[843299 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('input_training.csv')\n",
    "X.fillna(0, inplace=True)\n",
    "X.sort_values(by=\"ID\",inplace=True)\n",
    "y = pd.read_csv(r'output\\output_training_gmEd6Zt.csv')\n",
    "train = pd.merge(X,y,on=\"ID\").copy()\n",
    "train.set_index(\"ID\", inplace=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>r7</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>1000384</td>\n",
       "      <td>1000064</td>\n",
       "      <td>79.19</td>\n",
       "      <td>-26.37</td>\n",
       "      <td>-167.18</td>\n",
       "      <td>103.46</td>\n",
       "      <td>-102.27</td>\n",
       "      <td>-198.02</td>\n",
       "      <td>13.77</td>\n",
       "      <td>-59.61</td>\n",
       "      <td>...</td>\n",
       "      <td>29.82</td>\n",
       "      <td>-29.69</td>\n",
       "      <td>77.57</td>\n",
       "      <td>23.75</td>\n",
       "      <td>82.94</td>\n",
       "      <td>-17.63</td>\n",
       "      <td>17.60</td>\n",
       "      <td>5.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000001</th>\n",
       "      <td>1000148</td>\n",
       "      <td>1000841</td>\n",
       "      <td>-321.77</td>\n",
       "      <td>-178.53</td>\n",
       "      <td>107.97</td>\n",
       "      <td>27.29</td>\n",
       "      <td>-64.54</td>\n",
       "      <td>-73.78</td>\n",
       "      <td>9.93</td>\n",
       "      <td>-26.50</td>\n",
       "      <td>...</td>\n",
       "      <td>1.53</td>\n",
       "      <td>32.08</td>\n",
       "      <td>-33.50</td>\n",
       "      <td>-21.41</td>\n",
       "      <td>43.89</td>\n",
       "      <td>68.07</td>\n",
       "      <td>-23.20</td>\n",
       "      <td>-13.14</td>\n",
       "      <td>-82.95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000002</th>\n",
       "      <td>1000441</td>\n",
       "      <td>1000380</td>\n",
       "      <td>-51.95</td>\n",
       "      <td>-19.58</td>\n",
       "      <td>-26.16</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>3.29</td>\n",
       "      <td>-46.11</td>\n",
       "      <td>16.55</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.16</td>\n",
       "      <td>-40.68</td>\n",
       "      <td>13.61</td>\n",
       "      <td>3.40</td>\n",
       "      <td>6.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-10.19</td>\n",
       "      <td>-6.80</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000003</th>\n",
       "      <td>1000059</td>\n",
       "      <td>1001843</td>\n",
       "      <td>-169.49</td>\n",
       "      <td>9.57</td>\n",
       "      <td>-76.48</td>\n",
       "      <td>28.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.21</td>\n",
       "      <td>105.87</td>\n",
       "      <td>-9.52</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.72</td>\n",
       "      <td>-9.73</td>\n",
       "      <td>-9.74</td>\n",
       "      <td>29.24</td>\n",
       "      <td>-29.18</td>\n",
       "      <td>34.15</td>\n",
       "      <td>14.58</td>\n",
       "      <td>-29.13</td>\n",
       "      <td>4.87</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000004</th>\n",
       "      <td>1000336</td>\n",
       "      <td>1001783</td>\n",
       "      <td>-46.19</td>\n",
       "      <td>32.17</td>\n",
       "      <td>-3.08</td>\n",
       "      <td>-32.06</td>\n",
       "      <td>11.75</td>\n",
       "      <td>27.81</td>\n",
       "      <td>-13.55</td>\n",
       "      <td>-30.21</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.84</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>1.23</td>\n",
       "      <td>5.53</td>\n",
       "      <td>-7.37</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-4.30</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885794</th>\n",
       "      <td>1000125</td>\n",
       "      <td>1001131</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.87</td>\n",
       "      <td>-67.41</td>\n",
       "      <td>-18.15</td>\n",
       "      <td>12.99</td>\n",
       "      <td>-42.80</td>\n",
       "      <td>29.96</td>\n",
       "      <td>35.06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.09</td>\n",
       "      <td>-12.73</td>\n",
       "      <td>5.10</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>-20.41</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7.67</td>\n",
       "      <td>6.40</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885795</th>\n",
       "      <td>1000314</td>\n",
       "      <td>1000333</td>\n",
       "      <td>76.29</td>\n",
       "      <td>-65.02</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-24.54</td>\n",
       "      <td>-17.78</td>\n",
       "      <td>13.70</td>\n",
       "      <td>-19.17</td>\n",
       "      <td>24.70</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>-13.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.10</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>10.94</td>\n",
       "      <td>10.93</td>\n",
       "      <td>8.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885796</th>\n",
       "      <td>1000247</td>\n",
       "      <td>1000348</td>\n",
       "      <td>-73.66</td>\n",
       "      <td>37.11</td>\n",
       "      <td>-7.92</td>\n",
       "      <td>-7.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-18.49</td>\n",
       "      <td>-26.46</td>\n",
       "      <td>13.27</td>\n",
       "      <td>...</td>\n",
       "      <td>10.63</td>\n",
       "      <td>5.31</td>\n",
       "      <td>-10.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.28</td>\n",
       "      <td>-5.31</td>\n",
       "      <td>6.64</td>\n",
       "      <td>7.96</td>\n",
       "      <td>5.30</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885797</th>\n",
       "      <td>1000035</td>\n",
       "      <td>1000040</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-102.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885798</th>\n",
       "      <td>1000452</td>\n",
       "      <td>1000260</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1.36</td>\n",
       "      <td>-6.81</td>\n",
       "      <td>-5.45</td>\n",
       "      <td>4.09</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.17</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.08</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.72</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>5.44</td>\n",
       "      <td>9.51</td>\n",
       "      <td>-5.43</td>\n",
       "      <td>-4.07</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>885799 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             day   equity      r0      r1      r2      r3      r4      r5  \\\n",
       "ID                                                                          \n",
       "1000000  1000384  1000064   79.19  -26.37 -167.18  103.46 -102.27 -198.02   \n",
       "1000001  1000148  1000841 -321.77 -178.53  107.97   27.29  -64.54  -73.78   \n",
       "1000002  1000441  1000380  -51.95  -19.58  -26.16  -26.28   -3.29    3.29   \n",
       "1000003  1000059  1001843 -169.49    9.57  -76.48   28.90    0.00  -19.21   \n",
       "1000004  1000336  1001783  -46.19   32.17   -3.08  -32.06   11.75   27.81   \n",
       "...          ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "1885794  1000125  1001131    0.00   67.87  -67.41  -18.15   12.99  -42.80   \n",
       "1885795  1000314  1000333   76.29  -65.02    2.73  -24.54  -17.78   13.70   \n",
       "1885796  1000247  1000348  -73.66   37.11   -7.92   -7.93    0.00  -18.49   \n",
       "1885797  1000035  1000040    0.00 -102.74    0.00    0.00    0.00    0.00   \n",
       "1885798  1000452  1000260    6.81    1.36   -6.81   -5.45    4.09    8.17   \n",
       "\n",
       "             r6     r7  ...    r44    r45    r46    r47    r48    r49    r50  \\\n",
       "ID                      ...                                                    \n",
       "1000000   13.77 -59.61  ...  29.82 -29.69  77.57  23.75  82.94 -17.63  17.60   \n",
       "1000001    9.93 -26.50  ...   1.53  32.08 -33.50 -21.41  43.89  68.07 -23.20   \n",
       "1000002  -46.11  16.55  ... -10.16 -40.68  13.61   3.40   6.80  10.20   0.00   \n",
       "1000003  105.87  -9.52  ...  -9.72  -9.73  -9.74  29.24 -29.18  34.15  14.58   \n",
       "1000004  -13.55 -30.21  ...  -1.84  -1.23   1.23   5.53  -7.37   1.84   1.23   \n",
       "...         ...    ...  ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "1885794   29.96  35.06  ...   5.09 -12.73   5.10  -8.91 -20.41   2.56   0.00   \n",
       "1885795  -19.17  24.70  ...  -1.36 -13.65   0.00   0.00  -4.10  -2.73  10.94   \n",
       "1885796  -26.46  13.27  ...  10.63   5.31 -10.61   0.00  13.28  -5.31   6.64   \n",
       "1885797    0.00   0.00  ...   0.00   0.00   0.00   0.00   0.00   0.00   0.00   \n",
       "1885798    0.00   8.17  ...  -4.08  -2.72   1.36   2.72  -1.36   5.44   9.51   \n",
       "\n",
       "           r51    r52  reod  \n",
       "ID                           \n",
       "1000000   5.87   0.00     0  \n",
       "1000001 -13.14 -82.95     1  \n",
       "1000002 -10.19  -6.80     1  \n",
       "1000003 -29.13   4.87    -1  \n",
       "1000004   0.61  -4.30    -1  \n",
       "...        ...    ...   ...  \n",
       "1885794  -7.67   6.40    -1  \n",
       "1885795  10.93   8.18     1  \n",
       "1885796   7.96   5.30    -1  \n",
       "1885797   0.00   0.00    -1  \n",
       "1885798  -5.43  -4.07    -1  \n",
       "\n",
       "[885799 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(r'input_test.csv')\n",
    "X.fillna(0, inplace=True)\n",
    "X.sort_values(by=\"ID\",inplace=True)\n",
    "y = pd.read_csv(r\"output\\output_test_random.csv\")\n",
    "test = pd.merge(X,y,on=\"ID\").copy()\n",
    "test.set_index(\"ID\", inplace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=[\"reod\"])\n",
    "y_train = train[\"reod\"]\n",
    "X_test = test.drop(columns=[\"reod\"])\n",
    "y_test = test[\"reod\"]\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rabhi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 971us/step - accuracy: 0.4118 - loss: -452.6877 - val_accuracy: 0.4137 - val_loss: -7072.1079\n",
      "Epoch 2/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4119 - loss: -14964.4561 - val_accuracy: 0.4138 - val_loss: -49779.1758\n",
      "Epoch 3/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -71030.5859 - val_accuracy: 0.4137 - val_loss: -158226.6094\n",
      "Epoch 4/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4120 - loss: -202229.2812 - val_accuracy: 0.4139 - val_loss: -362174.7500\n",
      "Epoch 5/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 998us/step - accuracy: 0.4128 - loss: -434963.7812 - val_accuracy: 0.4147 - val_loss: -690621.5625\n",
      "Epoch 6/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -809043.5625 - val_accuracy: 0.4137 - val_loss: -1173467.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1ms/step - accuracy: 0.4125 - loss: -1276120.8750 - val_accuracy: 0.4139 - val_loss: -1837720.2500\n",
      "Epoch 8/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1ms/step - accuracy: 0.4115 - loss: -2066418.5000 - val_accuracy: 0.4140 - val_loss: -2712738.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1ms/step - accuracy: 0.4129 - loss: -2960895.2500 - val_accuracy: 0.4146 - val_loss: -3816551.5000\n",
      "Epoch 10/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - accuracy: 0.4122 - loss: -4175708.7500 - val_accuracy: 0.4137 - val_loss: -5194381.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4128 - loss: -5842845.0000 - val_accuracy: 0.4138 - val_loss: -6881469.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1ms/step - accuracy: 0.4133 - loss: -7459750.5000 - val_accuracy: 0.4144 - val_loss: -8892044.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4125 - loss: -9665605.0000 - val_accuracy: 0.4149 - val_loss: -11249667.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4131 - loss: -11995633.0000 - val_accuracy: 0.4136 - val_loss: -13981548.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -14240356.0000 - val_accuracy: 0.4145 - val_loss: -17122564.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4121 - loss: -19001588.0000 - val_accuracy: 0.4136 - val_loss: -20701416.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4122 - loss: -21323864.0000 - val_accuracy: 0.4135 - val_loss: -24778498.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -26317236.0000 - val_accuracy: 0.4142 - val_loss: -29356078.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4121 - loss: -31155534.0000 - val_accuracy: 0.4154 - val_loss: -34428848.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4121 - loss: -34947844.0000 - val_accuracy: 0.4135 - val_loss: -40094784.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4124 - loss: -41457448.0000 - val_accuracy: 0.4140 - val_loss: -46350552.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4128 - loss: -47676172.0000 - val_accuracy: 0.4139 - val_loss: -53216912.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4127 - loss: -55998244.0000 - val_accuracy: 0.4136 - val_loss: -60716928.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4123 - loss: -61171436.0000 - val_accuracy: 0.4139 - val_loss: -68885072.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4111 - loss: -70003304.0000 - val_accuracy: 0.4134 - val_loss: -77709184.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4137 - loss: -77874600.0000 - val_accuracy: 0.4137 - val_loss: -87306720.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4113 - loss: -90196824.0000 - val_accuracy: 0.4136 - val_loss: -97652376.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4124 - loss: -100339008.0000 - val_accuracy: 0.4177 - val_loss: -108742168.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4132 - loss: -110036800.0000 - val_accuracy: 0.4135 - val_loss: -120693288.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4121 - loss: -120997264.0000 - val_accuracy: 0.4137 - val_loss: -133491984.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4131 - loss: -130893752.0000 - val_accuracy: 0.4139 - val_loss: -147139792.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4120 - loss: -148850704.0000 - val_accuracy: 0.4141 - val_loss: -161789712.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4131 - loss: -172948864.0000 - val_accuracy: 0.4143 - val_loss: -177241792.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4122 - loss: -188615856.0000 - val_accuracy: 0.4135 - val_loss: -193687584.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4118 - loss: -193528560.0000 - val_accuracy: 0.4155 - val_loss: -211094928.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4115 - loss: -219281728.0000 - val_accuracy: 0.4135 - val_loss: -229508304.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -236480496.0000 - val_accuracy: 0.4135 - val_loss: -249098752.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4113 - loss: -251074016.0000 - val_accuracy: 0.4146 - val_loss: -269539136.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4112 - loss: -265845136.0000 - val_accuracy: 0.4137 - val_loss: -291124992.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4124 - loss: -302572608.0000 - val_accuracy: 0.4198 - val_loss: -313892736.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step - accuracy: 0.4129 - loss: -322223808.0000 - val_accuracy: 0.4136 - val_loss: -337852640.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -341163904.0000 - val_accuracy: 0.4147 - val_loss: -362850304.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - accuracy: 0.4130 - loss: -365226304.0000 - val_accuracy: 0.4141 - val_loss: -389175520.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4131 - loss: -398828576.0000 - val_accuracy: 0.4145 - val_loss: -416594496.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4133 - loss: -425779776.0000 - val_accuracy: 0.4137 - val_loss: -445391104.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - accuracy: 0.4120 - loss: -448860352.0000 - val_accuracy: 0.4138 - val_loss: -475348672.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4131 - loss: -460674688.0000 - val_accuracy: 0.4137 - val_loss: -506732864.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4130 - loss: -502212576.0000 - val_accuracy: 0.4148 - val_loss: -539388928.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4118 - loss: -558950080.0000 - val_accuracy: 0.4137 - val_loss: -573428416.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - accuracy: 0.4126 - loss: -562728192.0000 - val_accuracy: 0.4136 - val_loss: -608943168.0000\n",
      "\u001b[1m27682/27682\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 758us/step - accuracy: 0.4112 - loss: -37659496448.0000\n",
      "Test Accuracy: 41.08%\n",
      "\u001b[1m27682/27682\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 514us/step\n",
      "Accuracy Score: 0.41077603384063427\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.8425 - val_loss: 0.7652\n",
      "Epoch 2/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7960 - val_loss: 0.7624\n",
      "Epoch 3/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8010 - val_loss: 0.7577\n",
      "Epoch 4/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8110 - val_loss: 0.7561\n",
      "Epoch 5/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - loss: 0.7783 - val_loss: 0.7557\n",
      "Epoch 6/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.8249 - val_loss: 0.7569\n",
      "Epoch 7/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8549 - val_loss: 0.7494\n",
      "Epoch 8/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7923 - val_loss: 0.7487\n",
      "Epoch 9/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8049 - val_loss: 0.7485\n",
      "Epoch 10/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8097 - val_loss: 0.7470\n",
      "Epoch 11/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7704 - val_loss: 0.7925\n",
      "Epoch 12/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7732 - val_loss: 0.7441\n",
      "Epoch 13/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - loss: 0.8319 - val_loss: 0.7428\n",
      "Epoch 14/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7789 - val_loss: 0.7444\n",
      "Epoch 15/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7467 - val_loss: 0.7436\n",
      "Epoch 16/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7560 - val_loss: 0.7453\n",
      "Epoch 17/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7274 - val_loss: 0.7428\n",
      "Epoch 18/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7427 - val_loss: 0.7434\n",
      "Epoch 19/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7443 - val_loss: 0.7451\n",
      "Epoch 20/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8213 - val_loss: 0.7432\n",
      "Epoch 21/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7511 - val_loss: 0.7411\n",
      "Epoch 22/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8073 - val_loss: 0.7418\n",
      "Epoch 23/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1ms/step - loss: 0.7600 - val_loss: 0.7417\n",
      "Epoch 24/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7515 - val_loss: 0.7410\n",
      "Epoch 25/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7468 - val_loss: 0.7415\n",
      "Epoch 26/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - loss: 0.8311 - val_loss: 0.7412\n",
      "Epoch 27/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - loss: 0.7628 - val_loss: 0.7416\n",
      "Epoch 28/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - loss: 0.7950 - val_loss: 0.7411\n",
      "Epoch 29/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - loss: 0.7984 - val_loss: 0.7583\n",
      "Epoch 30/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7409 - val_loss: 0.7412\n",
      "Epoch 31/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - loss: 0.7609 - val_loss: 0.7404\n",
      "Epoch 32/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1ms/step - loss: 0.7806 - val_loss: 0.7404\n",
      "Epoch 33/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - loss: 0.7646 - val_loss: 0.7406\n",
      "Epoch 34/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.8425 - val_loss: 0.7407\n",
      "Epoch 35/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step - loss: 0.8286 - val_loss: 0.7393\n",
      "Epoch 36/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7155 - val_loss: 0.7421\n",
      "Epoch 37/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7834 - val_loss: 0.7402\n",
      "Epoch 38/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7662 - val_loss: 0.7402\n",
      "Epoch 39/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7978 - val_loss: 0.7388\n",
      "Epoch 40/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7878 - val_loss: 0.7406\n",
      "Epoch 41/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7983 - val_loss: 0.7405\n",
      "Epoch 42/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - loss: 0.7950 - val_loss: 0.7389\n",
      "Epoch 43/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - loss: 0.7821 - val_loss: 0.7394\n",
      "Epoch 44/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7833 - val_loss: 0.7408\n",
      "Epoch 45/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1ms/step - loss: 0.7636 - val_loss: 0.7419\n",
      "Epoch 46/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7171 - val_loss: 0.7412\n",
      "Epoch 47/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7402 - val_loss: 0.7398\n",
      "Epoch 48/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7443 - val_loss: 0.7396\n",
      "Epoch 49/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - loss: 0.7143 - val_loss: 0.7394\n",
      "Epoch 50/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - loss: 0.7931 - val_loss: 0.7392\n",
      "\u001b[1m26354/26354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 469us/step\n",
      "\u001b[1m27682/27682\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 452us/step\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rabhi\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.4112 - loss: -34633.8594 - val_accuracy: 0.4135 - val_loss: -715911.8125\n",
      "Epoch 2/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3ms/step - accuracy: 0.4115 - loss: -921463.3125 - val_accuracy: 0.4135 - val_loss: -6152991.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3ms/step - accuracy: 0.4115 - loss: -4381888.0000 - val_accuracy: 0.4135 - val_loss: -23227044.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 3ms/step - accuracy: 0.4121 - loss: -31889424.0000 - val_accuracy: 0.4135 - val_loss: -69139440.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6ms/step - accuracy: 0.4111 - loss: -116310040.0000 - val_accuracy: 0.4135 - val_loss: -150079264.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 6ms/step - accuracy: 0.4105 - loss: -193050128.0000 - val_accuracy: 0.4135 - val_loss: -298042528.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6ms/step - accuracy: 0.4112 - loss: -354766304.0000 - val_accuracy: 0.4135 - val_loss: -518328992.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4ms/step - accuracy: 0.4119 - loss: -745466048.0000 - val_accuracy: 0.4135 - val_loss: -864638592.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3ms/step - accuracy: 0.4113 - loss: 110295568.0000 - val_accuracy: 0.4135 - val_loss: -1316811264.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step - accuracy: 0.4103 - loss: -2557968384.0000 - val_accuracy: 0.4135 - val_loss: -1985878016.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 3ms/step - accuracy: 0.4111 - loss: 447458656.0000 - val_accuracy: 0.4135 - val_loss: -2782533120.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3ms/step - accuracy: 0.4124 - loss: -2595517696.0000 - val_accuracy: 0.4135 - val_loss: -3958491392.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4ms/step - accuracy: 0.4110 - loss: -4510416896.0000 - val_accuracy: 0.4135 - val_loss: -5259468800.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3ms/step - accuracy: 0.4124 - loss: -762813952.0000 - val_accuracy: 0.4135 - val_loss: -6974881280.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3ms/step - accuracy: 0.4115 - loss: -7288545792.0000 - val_accuracy: 0.4135 - val_loss: -9220490240.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3ms/step - accuracy: 0.4121 - loss: -12199678976.0000 - val_accuracy: 0.4135 - val_loss: -12078416896.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 5ms/step - accuracy: 0.4113 - loss: -2408629504.0000 - val_accuracy: 0.4135 - val_loss: -14921724928.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 4ms/step - accuracy: 0.4124 - loss: -18420447232.0000 - val_accuracy: 0.4135 - val_loss: -18605932544.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3ms/step - accuracy: 0.4123 - loss: -23310583808.0000 - val_accuracy: 0.4135 - val_loss: -23014731776.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - accuracy: 0.4120 - loss: -31382999040.0000 - val_accuracy: 0.4135 - val_loss: -28499499008.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1ms/step - accuracy: 0.4112 - loss: -26918643712.0000 - val_accuracy: 0.4135 - val_loss: -33730060288.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4115 - loss: -38048780288.0000 - val_accuracy: 0.4135 - val_loss: -40741408768.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4108 - loss: -57924272128.0000 - val_accuracy: 0.4135 - val_loss: -47510093824.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - accuracy: 0.4115 - loss: -44974673920.0000 - val_accuracy: 0.4135 - val_loss: -55108505600.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1ms/step - accuracy: 0.4117 - loss: -70580682752.0000 - val_accuracy: 0.4135 - val_loss: -64808894464.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4114 - loss: 84296769536.0000 - val_accuracy: 0.4135 - val_loss: -75802542080.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4127 - loss: -85804638208.0000 - val_accuracy: 0.4135 - val_loss: -87721459712.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4103 - loss: -56511135744.0000 - val_accuracy: 0.4135 - val_loss: -101323907072.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -134642081792.0000 - val_accuracy: 0.4135 - val_loss: -117138939904.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4123 - loss: -99698638848.0000 - val_accuracy: 0.4135 - val_loss: -133653987328.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -134642466816.0000 - val_accuracy: 0.4135 - val_loss: -151452499968.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4132 - loss: -238835302400.0000 - val_accuracy: 0.4135 - val_loss: -171354570752.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4110 - loss: -106422697984.0000 - val_accuracy: 0.4135 - val_loss: -193333657600.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4114 - loss: -40355074048.0000 - val_accuracy: 0.4135 - val_loss: -215588438016.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4118 - loss: -109848813568.0000 - val_accuracy: 0.4135 - val_loss: -241862459392.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4117 - loss: -308344815616.0000 - val_accuracy: 0.4135 - val_loss: -271615967232.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4125 - loss: -217257656320.0000 - val_accuracy: 0.4135 - val_loss: -300662521856.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -243677298688.0000 - val_accuracy: 0.4135 - val_loss: -332352815104.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - accuracy: 0.4118 - loss: -318113021952.0000 - val_accuracy: 0.4135 - val_loss: -367441281024.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4123 - loss: -464004055040.0000 - val_accuracy: 0.4135 - val_loss: -414444978176.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - accuracy: 0.4122 - loss: -581466587136.0000 - val_accuracy: 0.4135 - val_loss: -450523267072.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4124 - loss: -709153521664.0000 - val_accuracy: 0.4135 - val_loss: -491450302464.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4115 - loss: -643189112832.0000 - val_accuracy: 0.4135 - val_loss: -538130055168.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - accuracy: 0.4112 - loss: -347383070720.0000 - val_accuracy: 0.4135 - val_loss: -582881116160.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step - accuracy: 0.4105 - loss: -803185360896.0000 - val_accuracy: 0.4135 - val_loss: -639585550336.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4117 - loss: -426617470976.0000 - val_accuracy: 0.4135 - val_loss: -692597751808.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4117 - loss: -985645187072.0000 - val_accuracy: 0.4135 - val_loss: -748754370560.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -917903179776.0000 - val_accuracy: 0.4135 - val_loss: -814424981504.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4114 - loss: 123605032960.0000 - val_accuracy: 0.4135 - val_loss: -877174652928.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m21083/21083\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.4116 - loss: -1112890671104.0000 - val_accuracy: 0.4135 - val_loss: -946731810816.0000\n",
      "\u001b[1m27682/27682\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 739us/step - accuracy: 0.4112 - loss: -2175204440670208.0000\n",
      "Test Accuracy: 41.08%\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(128, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "decoded = Dense(128, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_test_encoded = encoder.predict(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=X_train_encoded.shape[1], activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_encoded, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test_encoded, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Distribution de y_test en pourcentage'}, xlabel='reod'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(y_test.value_counts(normalize=True) * 100).plot(kind='bar', title=\"Distribution de y_test en pourcentage\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
