{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Sujet\n",
    "\n",
    "## 1.1) Contexte\n",
    "Dans le cadre de ce challenge (issu du Collège de France ou du Lab Banque de France), nous disposons de données de marché avec pour objectif de prédire la direction du prix (baisse, stable, hausse) en fin de journée, à partir des rendements du matin. \n",
    "\n",
    "Ce marché américain étant particulièrement liquide, l’enjeu est de pouvoir estimer la tendance entre 14h et 16h pour prendre des décisions d’investissement ou d’arbitrage.\n",
    "\n",
    "## 1.2) Description des données\n",
    "- **Index des données**  \n",
    "  - Chaque ligne représente un jour donné et une action donnée (identifiants : `day` et `equity`).  \n",
    "  - Les colonnes `r0` à `r52` correspondent aux rendements (en points de base) toutes les 5 minutes entre 9h30 et 14h.  \n",
    "\n",
    "- **Variables explicatives** :  \n",
    "  - Les 53 rendements (`r0, r1, …, r52`), éventuellement d’autres features dérivées.\n",
    "\n",
    "- **Variable cible** :  \n",
    "  - `reod` {-1, 0, 1\\} indiquant la tendance de l’actif entre 14h et 16h (baisse, stable ou hausse).\n",
    "\n",
    "## 1.3) Description du benchmark\n",
    "Le benchmark mentionné propose un re de référence autour de 33% (basé sur des prédictions aléatoires) et un modèle plus élaboré atteint environ 41,74%. Notre objectif est de dépasser ce score en construisant un pipeline de Machine Learning robuste et optimisé.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 2) Problématique\n",
    "Notre mission est de prédire la classe de rendement (`reod`) en fin de journée, à partir des données matinales. Les défis principaux sont :\n",
    "- La **taille importante** du dataset (plusieurs centaines de milliers de lignes).  \n",
    "- La **présence de valeurs manquantes** (`NaN`).  \n",
    "- L’**absence de jours/actions communs** entre le jeu d’entraînement et le jeu de test, ce qui complique l’utilisation directe de `equity` ou `day` en tant que features.  \n",
    "- Le **risque de surcoût mémoire** et de temps de calcul avec certains modèles comme les Random Forest.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3) Préparation des données\n",
    "\n",
    "## 3.1) Importations des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'input_training.csv')\n",
    "X.sort_values(by=\"ID\",inplace=True)\n",
    "\n",
    "y = pd.read_csv(r'output\\output_training_gmEd6Zt.csv')\n",
    "\n",
    "input_test = pd.read_csv(r'input_test.csv')\n",
    "input_test.sort_values(by=\"ID\",inplace=True)\n",
    "\n",
    "\n",
    "output_random_bench = pd.read_csv(r\"output\\output_test_random.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Exploration des données\n",
    "\n",
    "1. **Analyse des valeurs manquantes** : taux de NaN par colonne, par ligne, distribution des NaN.\n",
    "2. **Statistiques descriptives** (moyenne, écart-type, quantiles) sur les rendements.  \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "(X.isna().sum()).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ligne totale du dataset 843299\n",
      "nombre de lignes contenant au moins des NaN 242363, soit en %: 28.739865694136956\n",
      "nombre de lignes avec plus 30% de NaN dans une colonne 80786, soit en % 14.89898600614966\n",
      "nombre de lignes avec plus 50% de NaN dans une colonne 80786, soit en % 9.579757594874415\n"
     ]
    }
   ],
   "source": [
    "col_rendements = [col for col in X.columns if col.startswith(\"r\")]\n",
    "\n",
    "NaN_analysis = pd.DataFrame()\n",
    "NaN_analysis[\"NaN_count\"] = X[col_rendements].isna().sum(axis=1)\n",
    "\n",
    "nombre_colonnes = len(col_rendements)\n",
    "NaN_analysis[\"NaN_percent\"] = (NaN_analysis[\"NaN_count\"]/nombre_colonnes)*100\n",
    "\n",
    "nbr_row_na = X.isna().any(axis=1).sum()\n",
    "\n",
    "print(f\"Nombre de ligne totale du dataset {len(X)}\")\n",
    "print(f\"nombre de lignes contenant au moins des NaN {nbr_row_na}, soit en %: {(nbr_row_na/len(X)*100)}\")\n",
    "print(f\"nombre de lignes avec plus 30% de NaN dans une colonne {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>50])}, soit en % {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>30])/len(X)*100}\")\n",
    "print(f\"nombre de lignes avec plus 50% de NaN dans une colonne {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>50])}, soit en % {len(NaN_analysis[NaN_analysis[\"NaN_percent\"]>50])/len(X)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de lignes data input train 843299\n",
      "Nombre de lignes output_training 843299\n",
      "Nombre de lignes data input test 885799\n",
      "Nombre de lignes de l'output random bench 885799\n"
     ]
    }
   ],
   "source": [
    "print(f\"nombre de lignes data input train {len(X)}\")\n",
    "print(f\"Nombre de lignes output_training {len(y)}\")\n",
    "print(f\"Nombre de lignes data input test {len(input_test)}\")\n",
    "print(f\"Nombre de lignes de l'output random bench {len(output_random_bench)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>1488</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>302</td>\n",
       "      <td>513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843294</th>\n",
       "      <td>843294</td>\n",
       "      <td>297</td>\n",
       "      <td>123</td>\n",
       "      <td>3.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-70.34</td>\n",
       "      <td>74.24</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23.63</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-3.98</td>\n",
       "      <td>2.28</td>\n",
       "      <td>-21.62</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>9.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843295</th>\n",
       "      <td>843295</td>\n",
       "      <td>16</td>\n",
       "      <td>1501</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-183.49</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>46.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-39.60</td>\n",
       "      <td>13.25</td>\n",
       "      <td>...</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.42</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-19.88</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843296</th>\n",
       "      <td>843296</td>\n",
       "      <td>166</td>\n",
       "      <td>1231</td>\n",
       "      <td>37.02</td>\n",
       "      <td>2.93</td>\n",
       "      <td>-3.67</td>\n",
       "      <td>16.89</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>13.56</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.28</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>-3.65</td>\n",
       "      <td>-1.10</td>\n",
       "      <td>-13.51</td>\n",
       "      <td>2.92</td>\n",
       "      <td>-6.21</td>\n",
       "      <td>9.69</td>\n",
       "      <td>-3.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843297</th>\n",
       "      <td>843297</td>\n",
       "      <td>297</td>\n",
       "      <td>747</td>\n",
       "      <td>34.45</td>\n",
       "      <td>15.10</td>\n",
       "      <td>-35.61</td>\n",
       "      <td>19.25</td>\n",
       "      <td>-16.46</td>\n",
       "      <td>-26.12</td>\n",
       "      <td>20.68</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.52</td>\n",
       "      <td>-6.90</td>\n",
       "      <td>9.67</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.90</td>\n",
       "      <td>-11.04</td>\n",
       "      <td>33.16</td>\n",
       "      <td>13.77</td>\n",
       "      <td>12.38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843298</th>\n",
       "      <td>843298</td>\n",
       "      <td>493</td>\n",
       "      <td>920</td>\n",
       "      <td>-25.91</td>\n",
       "      <td>-43.37</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.42</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>...</td>\n",
       "      <td>8.75</td>\n",
       "      <td>-17.48</td>\n",
       "      <td>-13.13</td>\n",
       "      <td>-13.15</td>\n",
       "      <td>30.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-4.39</td>\n",
       "      <td>-8.79</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843299 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID  day  equity      r0      r1     r2     r3     r4     r5  \\\n",
       "0            0  249    1488    0.00     NaN    NaN    NaN   0.00    NaN   \n",
       "1            1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00   \n",
       "2            2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14   \n",
       "3            3  302     513    0.00     NaN   0.00   0.00   0.00    NaN   \n",
       "4            4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   \n",
       "...        ...  ...     ...     ...     ...    ...    ...    ...    ...   \n",
       "843294  843294  297     123    3.96    0.00 -70.34  74.24  -0.56   0.00   \n",
       "843295  843295   16    1501    0.00 -183.49 -13.19  46.24   0.00 -39.60   \n",
       "843296  843296  166    1231   37.02    2.93  -3.67  16.89  -4.03  13.56   \n",
       "843297  843297  297     747   34.45   15.10 -35.61  19.25 -16.46 -26.12   \n",
       "843298  843298  493     920  -25.91  -43.37   0.00  17.42   0.00   0.00   \n",
       "\n",
       "           r6  ...    r44    r45    r46    r47    r48    r49    r50    r51  \\\n",
       "0         NaN  ...   0.00    NaN   0.00    NaN   0.00    NaN    NaN    NaN   \n",
       "1       41.24  ... -16.92  -4.84   4.84   0.00   7.26  -9.68 -19.38   9.71   \n",
       "2       49.12  ...   1.59   6.37 -49.32  -9.59  -6.40  22.41  -6.39   7.99   \n",
       "3         NaN  ...    NaN    NaN    NaN    NaN    NaN    NaN    NaN   0.00   \n",
       "4        0.00  ... -21.44 -21.48  10.78 -21.55  -5.40 -10.81   5.41 -32.47   \n",
       "...       ...  ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "843294 -23.63  ...   1.71   0.00  -3.98   2.28 -21.62  -1.71   9.12   0.00   \n",
       "843295  13.25  ...   6.62   0.00  19.85   0.00 -26.42   6.62   0.00   0.00   \n",
       "843296  -4.39  ...  -3.28  -1.46  -3.65  -1.10 -13.51   2.92  -6.21   9.69   \n",
       "843297  20.68  ...  -5.52  -6.90   9.67   1.38   6.90 -11.04  33.16  13.77   \n",
       "843298  21.74  ...   8.75 -17.48 -13.13 -13.15  30.74   0.00   0.00  -4.39   \n",
       "\n",
       "          r52  reod  \n",
       "0        0.00     0  \n",
       "1       26.68     0  \n",
       "2       15.96    -1  \n",
       "3         NaN     0  \n",
       "4       43.43    -1  \n",
       "...       ...   ...  \n",
       "843294   9.11     1  \n",
       "843295 -19.88    -1  \n",
       "843296  -3.66     0  \n",
       "843297  12.38     1  \n",
       "843298  -8.79    -1  \n",
       "\n",
       "[843299 rows x 57 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.merge(X,y,on=\"ID\")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Benchmark (modèle simple)\n",
    "\n",
    "## 4.1) Expérience avec Random Forest\n",
    "- Nous avons d’abord tenté un **RandomForestClassifier** (pipeline basique + imputation).  \n",
    "- Sur de grands volumes de données, nous avons rencontré des **erreurs de mémoire** (MemoryError) après plusieurs heures d’entraînement, montrant que cette approche n’était pas adaptée dans notre configuration (en particulier si `n_estimators` et `max_depth` sont trop élevés). (Ca a tourné 5H avant de gentiment me dire d'aller voir ailleurs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rendements = pd.DataFrame(X.loc[:,\"r0\":\"r52\"].fillna(0))\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rendements,y,test_size=0.1, random_state=42)\n",
    "\n",
    "#On crée un pipeline :\n",
    "pipeline_bench = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=10,max_depth=10,random_state=42, n_jobs=1)),\n",
    "]) # Avant essai avec 50 estimators\n",
    "\n",
    "pipeline_bench.fit(X_train,y_train)\n",
    "\n",
    "y_pred = pipeline_bench.predict(X_val)\n",
    "\n",
    "print(\"Accuracy\", accuracy_score(y_val,y_pred))\n",
    "print(\"Matrice de confusion : \\n\", confusion_matrix(y_val, y_pred))\n",
    "print(\"Rapport de classification: \\n\", classification_report(y_val,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2) Pourquoi s’orienter vers XGBoost ?\n",
    "- **XGBoost** (ou LightGBM) est un algorithme de **gradient boosting** très optimisé pour les données tabulaires volumineuses.  \n",
    "- Il gère mieux la mémoire, propose des méthodes de recherche de split plus efficaces et peut gérer des valeurs manquantes.  \n",
    "- Expérience terrain et compétitions : XGBoost est souvent **plus rapide** et **plus performant** qu’une Random Forest sur de gros datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1  0  1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#On crée un pipeline :\u001b[39;00m\n\u001b[0;32m     18\u001b[0m pipeline_bench \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     19\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m\"\u001b[39m, StandardScaler()),\n\u001b[0;32m     20\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m     21\u001b[0m ]) \n\u001b[1;32m---> 23\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     26\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:660\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    655\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    656\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    657\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    658\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    659\u001b[0m         )\n\u001b[1;32m--> 660\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\julie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1559\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     expected_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1556\u001b[0m     classes\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m expected_classes\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (classes \u001b[38;5;241m==\u001b[39m expected_classes)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m   1558\u001b[0m ):\n\u001b[1;32m-> 1559\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid classes inferred from unique values of `y`.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m     )\n\u001b[0;32m   1564\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_xgb_params()\n\u001b[0;32m   1566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0 1 2], got [-1  0  1]"
     ]
    }
   ],
   "source": [
    "X_rendements = pd.DataFrame(X.loc[:,\"r0\":\"r52\"].fillna(0))\n",
    "y_rendements = X_train[\"reod\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rendements,y_rendements,test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = XGBClassifier(\n",
    "    objective='multi:softmax',  # pour classification multi-classes\n",
    "    num_class=3,               # si vous avez 3 classes : -1, 0, +1\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax', num_class=3, random_state=42))\n",
    "])\n",
    "#On crée un pipeline :\n",
    "pipeline_bench = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=10,max_depth=10,random_state=42, n_jobs=-1)),\n",
    "]) \n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy :\", accuracy)\n",
    "\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ce que nous comptons faire\n",
    "\n",
    "Nous allons explorer plusieurs approches afin d'optimiser notre pipeline de prédiction. Voici notre plan d'expérimentation et d'optimisation :\n",
    "\n",
    "1. **Création de plusieurs jeux de données :**  \n",
    "   - **Dataset complet :** Conserver toutes les observations, même celles contenant des valeurs manquantes (NA).  \n",
    "   - **Dataset filtré :** Retirer les lignes où plus de 30% des colonnes présentent des NA.\n",
    "   - **Stratégies d'imputation :** Tester différentes méthodes pour compléter les valeurs manquantes, telles que :\n",
    "     - Interpolation linéaire.\n",
    "     - Imputation par KNN.\n",
    "     - Imputation par stratégie simple (moyenne, médiane, etc.).\n",
    "\n",
    "2. **Feature Engineering et Encodage :**  \n",
    "   - **Extraction de statistiques à partir des rendements :** Calculer, par exemple, la moyenne, l'écart-type, la somme cumulée, etc.\n",
    "   - **Exploitation des variables catégorielles `equity` et `day` :**  \n",
    "     - Pour `equity` : Extraire des agrégats (moyenne historique, volatilité, etc.) pour construire un \"profil\" de l'action.\n",
    "     - Pour `day` : Extraire des caractéristiques globales (tendance générale du marché, effet de la veille, etc.) qui pourraient influencer la performance du jour. (calculer le nombre moyen de mouvement haussier pour un jour, mouvement baissier, neutre...) --> Permettrait de mettre en avant un effet bullish ou bearish. \n",
    "   - Comparer l’impact de ces transformations sur les performances du modèle.\n",
    "   Concernant l'encodage : nous n'avons pas encore trouvé de manière d'encoder les variables equity et day en tant que tel à part en calculant des statistiques / caractéristiques globales\n",
    "\n",
    "3. **Essais avec des modèles simples :**  \n",
    "   - Développer des modèles de base (XGBoost) sur chacun des jeux de données préparés.\n",
    "   - Évaluer les performances à l’aide de métriques telles que l’accuracy et la matrice de confusion.\n",
    "   - Vérifier l’impact des différentes stratégies d’imputation sur les résultats.\n",
    "\n",
    "4. **Passage à des modèles plus complexes :**  \n",
    "   - Utiilisation de réseaux de neurones : LSTM ou un simple MLP.\n",
    "\n",
    "5. **Optimisation des hyperparamètres :**  \n",
    "   - Utiliser la validation croisée (k-fold) pour obtenir une estimation robuste de la performance.\n",
    "   - Mettre en place une recherche d’hyperparamètres (avec GridSearchCV ou RandomizedSearchCV) pour optimiser :\n",
    "     - Le taux d’apprentissage (`learning_rate`).\n",
    "     - Le nombre d’arbres (`n_estimators`).\n",
    "     - La profondeur maximale (`max_depth`).\n",
    "     - Les taux de subsampling, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
