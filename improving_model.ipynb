{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook est un \"brouillon\" pour tenter d'améliorer le modèle. (globalement je vais faire ça comme un gros sagouin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "from utils.data_registry import DATASETS\n",
    "from utils.features import add_features\n",
    "from utils.benchmarks import get_models\n",
    "from utils.experiment_runner import run_experiment, display_experiment_result, add_result\n",
    "from utils.load_data import load_datasets\n",
    "from utils.graphic import analyze_distributions, compare_column_stats, analyze_normalization\n",
    "from utils.variance_analysis import perform_pca_analysis, perform_tsne_analysis, analyze_correlations, evaluate_feature_sets, analyze_feature_importance, select_best_features, summarize_feature_analysis\n",
    "from utils.preprocess_data import precompute_datasets_with_features\n",
    "\n",
    "# Create empty results tracker\n",
    "results_tracker = pd.DataFrame(columns=[\n",
    "    \"dataset\", \"dataset_description\", \"model\", \"model_description\", \n",
    "    \"features_added\", \"feature_sets\", \"accuracy\", \"precision_weighted\", \n",
    "    \"recall_weighted\", \"f1_weighted\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Charger les datasets\n",
    "imputed_datasets = load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on va se concentrer que sur un jeu de données, on se fiche un peu de la méthode d'imputation, la différence est probablement négligeable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aperçu des données d'entraînement ---\n",
      "Nombre de lignes train: 730784, Nombre de colonnes: 57\n",
      "Nombre de lignes test: 857641, Nombre de colonnes: 57\n",
      "\n",
      "Premières lignes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "      <th>reod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>343</td>\n",
       "      <td>1279</td>\n",
       "      <td>-26.91</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.52</td>\n",
       "      <td>-5.55</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-4.79</td>\n",
       "      <td>-5.59</td>\n",
       "      <td>6.39</td>\n",
       "      <td>-6.38</td>\n",
       "      <td>-5.59</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>185</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-30.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>26.37</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.62</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>13.20</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  day  equity      r0      r1     r2     r3     r4     r5     r6  ...  \\\n",
       "0   1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  ...   \n",
       "1   2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  ...   \n",
       "2   4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00  ...   \n",
       "3   5  343    1279  -26.91    4.76   9.52  -5.55  -7.14  -1.59  -7.14  ...   \n",
       "4   6  212     185    0.00  -30.67   4.40 -13.19  13.20  26.37   4.38  ...   \n",
       "\n",
       "     r44    r45    r46    r47   r48    r49    r50    r51    r52  reod  \n",
       "0 -16.92  -4.84   4.84   0.00  7.26  -9.68 -19.38   9.71  26.68     0  \n",
       "1   1.59   6.37 -49.32  -9.59 -6.40  22.41  -6.39   7.99  15.96    -1  \n",
       "2 -21.44 -21.48  10.78 -21.55 -5.40 -10.81   5.41 -32.47  43.43    -1  \n",
       "3   0.80  -3.19   3.99  -3.19 -4.79  -5.59   6.39  -6.38  -5.59     0  \n",
       "4   0.00   6.62  13.23   0.00  0.00   4.40  13.20  -4.40   4.40    -1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#On importe les données de base sans imputation (70%) juste pour avoir une référence en termes de dimensions \n",
    "# Importation des données d'entraînement\n",
    "X_train_70 = pd.read_csv(r\"processed_data\\X_train_70.csv\")\n",
    "\n",
    "# Importation des données de test : \n",
    "X_test_70 = pd.read_csv(r\"processed_data\\X_test_70.csv\")\n",
    "\n",
    "# Aperçu des données\n",
    "print(\"\\n--- Aperçu des données d'entraînement ---\")\n",
    "print(f\"Nombre de lignes train: {X_train_70.shape[0]}, Nombre de colonnes: {X_train_70.shape[1]}\")\n",
    "print(f\"Nombre de lignes test: {X_test_70.shape[0]}, Nombre de colonnes: {X_test_70.shape[1]}\")\n",
    "print(\"\\nPremières lignes:\")\n",
    "display(X_train_70.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aperçu des données d'entraînement ---\n",
      "Nombre de lignes train: 730784, Nombre de colonnes: 56\n",
      "Nombre de lignes test: 857641, Nombre de colonnes: 56\n",
      "\n",
      "Premières lignes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r43</th>\n",
       "      <th>r44</th>\n",
       "      <th>r45</th>\n",
       "      <th>r46</th>\n",
       "      <th>r47</th>\n",
       "      <th>r48</th>\n",
       "      <th>r49</th>\n",
       "      <th>r50</th>\n",
       "      <th>r51</th>\n",
       "      <th>r52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>-16.92</td>\n",
       "      <td>-4.84</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.26</td>\n",
       "      <td>-9.68</td>\n",
       "      <td>-19.38</td>\n",
       "      <td>9.71</td>\n",
       "      <td>26.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.37</td>\n",
       "      <td>1.59</td>\n",
       "      <td>6.37</td>\n",
       "      <td>-49.32</td>\n",
       "      <td>-9.59</td>\n",
       "      <td>-6.40</td>\n",
       "      <td>22.41</td>\n",
       "      <td>-6.39</td>\n",
       "      <td>7.99</td>\n",
       "      <td>15.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.36</td>\n",
       "      <td>-21.44</td>\n",
       "      <td>-21.48</td>\n",
       "      <td>10.78</td>\n",
       "      <td>-21.55</td>\n",
       "      <td>-5.40</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>5.41</td>\n",
       "      <td>-32.47</td>\n",
       "      <td>43.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>343</td>\n",
       "      <td>1279</td>\n",
       "      <td>-26.91</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.52</td>\n",
       "      <td>-5.55</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>3.99</td>\n",
       "      <td>-3.19</td>\n",
       "      <td>-4.79</td>\n",
       "      <td>-5.59</td>\n",
       "      <td>6.39</td>\n",
       "      <td>-6.38</td>\n",
       "      <td>-5.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>185</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-30.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>26.37</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.62</td>\n",
       "      <td>13.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.40</td>\n",
       "      <td>13.20</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  day  equity      r0      r1     r2     r3     r4     r5     r6  ...  \\\n",
       "0   1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  ...   \n",
       "1   2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  ...   \n",
       "2   4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00  ...   \n",
       "3   5  343    1279  -26.91    4.76   9.52  -5.55  -7.14  -1.59  -7.14  ...   \n",
       "4   6  212     185    0.00  -30.67   4.40 -13.19  13.20  26.37   4.38  ...   \n",
       "\n",
       "    r43    r44    r45    r46    r47   r48    r49    r50    r51    r52  \n",
       "0 -4.83 -16.92  -4.84   4.84   0.00  7.26  -9.68 -19.38   9.71  26.68  \n",
       "1 -6.37   1.59   6.37 -49.32  -9.59 -6.40  22.41  -6.39   7.99  15.96  \n",
       "2 -5.36 -21.44 -21.48  10.78 -21.55 -5.40 -10.81   5.41 -32.47  43.43  \n",
       "3 -0.80   0.80  -3.19   3.99  -3.19 -4.79  -5.59   6.39  -6.38  -5.59  \n",
       "4 -4.41   0.00   6.62  13.23   0.00  0.00   4.40  13.20  -4.40   4.40  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import du dataset qu'on va utiliser + sa version preprocessed \n",
    "x_train_ffbf = pd.read_csv(r\"processed_data\\X_train_ffbf.csv\")\n",
    "x_test_ffbf = pd.read_csv(r'processed_data\\X_test_ffbf.csv')\n",
    "\n",
    "y_train_ffbg = x_train_ffbf[\"reod\"].copy()\n",
    "y_test_ffbg = x_test_ffbf[\"reod\"].copy()\n",
    "\n",
    "x_train_ffbf = x_train_ffbf.loc[:,\"ID\":\"r52\"]\n",
    "x_test_ffbf = x_test_ffbf.loc[:,\"ID\":\"r52\"]\n",
    "\n",
    "print(\"\\n--- Aperçu des données d'entraînement ---\")\n",
    "print(f\"Nombre de lignes train: {x_train_ffbf.shape[0]}, Nombre de colonnes: {x_train_ffbf.shape[1]}\")\n",
    "print(f\"Nombre de lignes test: {x_test_ffbf.shape[0]}, Nombre de colonnes: {x_test_ffbf.shape[1]}\")\n",
    "print(\"\\nPremières lignes:\")\n",
    "display(x_train_ffbf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aperçu des données d'entraînement ---\n",
      "Nombre de lignes train: 730784, Nombre de colonnes: 75\n",
      "Nombre de lignes test: 857641, Nombre de colonnes: 75\n",
      "\n",
      "--- Contient des Na ? ---\n",
      "Nombre de NA train: 0\n",
      "Nombre de NA train: 0\n",
      "\n",
      "Premières lignes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>day</th>\n",
       "      <th>equity</th>\n",
       "      <th>r0</th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>r3</th>\n",
       "      <th>r4</th>\n",
       "      <th>r5</th>\n",
       "      <th>r6</th>\n",
       "      <th>...</th>\n",
       "      <th>r_pos_sum</th>\n",
       "      <th>r_neg_sum</th>\n",
       "      <th>r_roll_mean_5</th>\n",
       "      <th>r_roll_std_5</th>\n",
       "      <th>r_roll_mean_10</th>\n",
       "      <th>r_roll_std_10</th>\n",
       "      <th>r_roll_mean_20</th>\n",
       "      <th>r_roll_std_20</th>\n",
       "      <th>r_momentum_5</th>\n",
       "      <th>r_momentum_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "      <td>107</td>\n",
       "      <td>-9.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-12.21</td>\n",
       "      <td>46.44</td>\n",
       "      <td>34.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41.24</td>\n",
       "      <td>...</td>\n",
       "      <td>411.59</td>\n",
       "      <td>-303.83</td>\n",
       "      <td>2.918</td>\n",
       "      <td>17.927125</td>\n",
       "      <td>-0.716</td>\n",
       "      <td>13.660327</td>\n",
       "      <td>3.0335</td>\n",
       "      <td>11.110787</td>\n",
       "      <td>26.68</td>\n",
       "      <td>24.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>323</td>\n",
       "      <td>1063</td>\n",
       "      <td>49.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-26.64</td>\n",
       "      <td>-23.66</td>\n",
       "      <td>-22.14</td>\n",
       "      <td>49.12</td>\n",
       "      <td>...</td>\n",
       "      <td>402.29</td>\n",
       "      <td>-523.80</td>\n",
       "      <td>6.714</td>\n",
       "      <td>13.011273</td>\n",
       "      <td>-2.375</td>\n",
       "      <td>19.591007</td>\n",
       "      <td>-8.4885</td>\n",
       "      <td>23.371939</td>\n",
       "      <td>25.55</td>\n",
       "      <td>84.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>1465</td>\n",
       "      <td>-123.84</td>\n",
       "      <td>-115.18</td>\n",
       "      <td>-26.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.42</td>\n",
       "      <td>10.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>531.34</td>\n",
       "      <td>-939.58</td>\n",
       "      <td>0.032</td>\n",
       "      <td>27.909429</td>\n",
       "      <td>-5.889</td>\n",
       "      <td>21.856480</td>\n",
       "      <td>-3.4500</td>\n",
       "      <td>29.044273</td>\n",
       "      <td>64.98</td>\n",
       "      <td>64.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>343</td>\n",
       "      <td>1279</td>\n",
       "      <td>-26.91</td>\n",
       "      <td>4.76</td>\n",
       "      <td>9.52</td>\n",
       "      <td>-5.55</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-7.14</td>\n",
       "      <td>...</td>\n",
       "      <td>122.82</td>\n",
       "      <td>-202.09</td>\n",
       "      <td>-3.192</td>\n",
       "      <td>5.385919</td>\n",
       "      <td>-1.835</td>\n",
       "      <td>4.357074</td>\n",
       "      <td>-0.2795</td>\n",
       "      <td>5.405468</td>\n",
       "      <td>-2.40</td>\n",
       "      <td>-5.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>185</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-30.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>-13.19</td>\n",
       "      <td>13.20</td>\n",
       "      <td>26.37</td>\n",
       "      <td>4.38</td>\n",
       "      <td>...</td>\n",
       "      <td>193.74</td>\n",
       "      <td>-204.49</td>\n",
       "      <td>3.520</td>\n",
       "      <td>6.526255</td>\n",
       "      <td>3.304</td>\n",
       "      <td>6.337378</td>\n",
       "      <td>3.4195</td>\n",
       "      <td>6.772368</td>\n",
       "      <td>4.40</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  day  equity      r0      r1     r2     r3     r4     r5     r6  ...  \\\n",
       "0   1  272     107   -9.76    0.00 -12.21  46.44  34.08   0.00  41.24  ...   \n",
       "1   2  323    1063   49.85    0.00   0.00 -26.64 -23.66 -22.14  49.12  ...   \n",
       "2   4  123    1465 -123.84 -115.18 -26.44   0.00  42.42  10.56   0.00  ...   \n",
       "3   5  343    1279  -26.91    4.76   9.52  -5.55  -7.14  -1.59  -7.14  ...   \n",
       "4   6  212     185    0.00  -30.67   4.40 -13.19  13.20  26.37   4.38  ...   \n",
       "\n",
       "   r_pos_sum  r_neg_sum  r_roll_mean_5  r_roll_std_5  r_roll_mean_10  \\\n",
       "0     411.59    -303.83          2.918     17.927125          -0.716   \n",
       "1     402.29    -523.80          6.714     13.011273          -2.375   \n",
       "2     531.34    -939.58          0.032     27.909429          -5.889   \n",
       "3     122.82    -202.09         -3.192      5.385919          -1.835   \n",
       "4     193.74    -204.49          3.520      6.526255           3.304   \n",
       "\n",
       "   r_roll_std_10  r_roll_mean_20  r_roll_std_20  r_momentum_5  r_momentum_10  \n",
       "0      13.660327          3.0335      11.110787         26.68          24.26  \n",
       "1      19.591007         -8.4885      23.371939         25.55          84.04  \n",
       "2      21.856480         -3.4500      29.044273         64.98          64.82  \n",
       "3       4.357074         -0.2795       5.405468         -2.40          -5.59  \n",
       "4       6.337378          3.4195       6.772368          4.40           6.61  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train_ffbf_with_feature = pd.read_csv(r\"processed_data\\preprocessed\\X_train_ffbf_with_features.csv\")\n",
    "x_test_ffbf_with_feature = pd.read_csv(r'processed_data\\preprocessed\\X_test_ffbf_with_features.csv')\n",
    "\n",
    "y_train_ffbg_with_features = x_train_ffbf_with_feature[\"reod\"].copy()\n",
    "y_test_ffbg_with_features = x_test_ffbf_with_feature[\"reod\"].copy()\n",
    "\n",
    "train_na_count = x_train_ffbf_with_feature.isna().sum().sum()\n",
    "test_na_count = x_test_ffbf_with_feature.isna().sum().sum()\n",
    "\n",
    "print(\"\\n--- Aperçu des données d'entraînement ---\")\n",
    "print(f\"Nombre de lignes train: {x_train_ffbf_with_feature.shape[0]}, Nombre de colonnes: {x_train_ffbf_with_feature.shape[1]}\")\n",
    "print(f\"Nombre de lignes test: {x_test_ffbf_with_feature.shape[0]}, Nombre de colonnes: {x_test_ffbf_with_feature.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Contient des Na ? ---\")\n",
    "print(f\"Nombre de NA train: {train_na_count}\")\n",
    "print(f\"Nombre de NA train: {test_na_count}\")\n",
    "\n",
    "print(\"\\nPremières lignes:\")\n",
    "display(x_train_ffbf_with_feature.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets disponibles:\n",
      "- raw: Données brutes\n",
      "- ffbf: Données forward filled puis backward\n",
      "- bfff: Données backward filled puis forward\n",
      "- interp: Données interpolation linéaire puis bffff\n",
      "- mice: Données MICE imputer puis bfff\n",
      "- knn: Données knn imputer puis bfff\n",
      "- raw_with_features: Données brutes avec features\n",
      "- ffbf_with_features: Données forward filled puis backward avec features\n",
      "- bfff_with_features: Données backward filled puis forward avec features\n",
      "- interp_with_features: Données interpolation linéaire puis bffff avec features\n",
      "- mice_with_features: Données MICE imputer puis bfff avec features\n",
      "- knn_with_features: Données knn imputer puis bfff avec features\n",
      "\n",
      "Modèles disponibles:\n",
      "- xgboost_baseline: XGBoost de base\n",
      "- xgboost_tuned: XGBoost avec paramètres \n",
      "- rf_baseline: Baseline Random Forest model\n",
      "- logistic: Multinomial Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDatasets disponibles:\")\n",
    "for key, info in DATASETS.items():\n",
    "    print(f\"- {key}: {info['description']}\")\n",
    "\n",
    "print(\"\\nModèles disponibles:\")\n",
    "models = get_models()\n",
    "for key, info in models.items():\n",
    "    if 'description' in info:\n",
    "        print(f\"- {key}: {info['description']}\")\n",
    "    else:\n",
    "        print(f\"- {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparaison des stratégies d'imputation avec XGBoost ---\n",
      "\n",
      " Using StandardScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing ffbf imputation...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy without feature engineering: 0.3136\n",
      "\n",
      " Using MinMaxScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing ffbf imputation...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy without feature engineering: 0.3136\n",
      "\n",
      " Using RobustScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing ffbf imputation...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy without feature engineering: 0.3136\n",
      "\n",
      " Using QuantileTransformer(output_distribution='normal') - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing ffbf imputation...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy without feature engineering: 0.3127\n",
      "\n",
      "Testing ffbf with precomputed features...\n",
      "\n",
      " Using StandardScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy with precomputed features: 0.3088\n",
      "Improvement from feature engineering: -0.0048 (-0.48%)\n",
      "\n",
      " Using MinMaxScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy with precomputed features: 0.3080\n",
      "Improvement from feature engineering: -0.0056 (-0.56%)\n",
      "\n",
      " Using RobustScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy with precomputed features: 0.3095\n",
      "Improvement from feature engineering: -0.0041 (-0.41%)\n",
      "\n",
      " Using QuantileTransformer(output_distribution='normal') - Testing normalisation on ffbf with precomputed features...\n",
      "Aucune valeur manquante détectée.\n",
      "Accuracy with precomputed features: 0.3109\n",
      "Improvement from feature engineering: -0.0026 (-0.26%)\n",
      "\n",
      "--- Testing with row normalization ---\n",
      "\n",
      " Using StandardScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing normalisation on ffbf with precomputed features...\n",
      "Application de la normalisation par ligne des rendements...\n",
      "Aucune valeur manquante détectée.\n",
      "ffbf with row normalization: Accuracy = 0.3107\n",
      "Impact of row normalization: 0.0019 (0.19%)\n",
      "\n",
      " Using MinMaxScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing normalisation on ffbf with precomputed features...\n",
      "Application de la normalisation par ligne des rendements...\n",
      "Aucune valeur manquante détectée.\n",
      "ffbf with row normalization: Accuracy = 0.3091\n",
      "Impact of row normalization: 0.0003 (0.03%)\n",
      "\n",
      " Using RobustScaler() - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing normalisation on ffbf with precomputed features...\n",
      "Application de la normalisation par ligne des rendements...\n",
      "Aucune valeur manquante détectée.\n",
      "ffbf with row normalization: Accuracy = 0.3085\n",
      "Impact of row normalization: -0.0004 (-0.04%)\n",
      "\n",
      " Using QuantileTransformer(output_distribution='normal') - Testing normalisation on ffbf with precomputed features...\n",
      "\n",
      "Testing normalisation on ffbf with precomputed features...\n",
      "Application de la normalisation par ligne des rendements...\n",
      "Aucune valeur manquante détectée.\n",
      "ffbf with row normalization: Accuracy = 0.3090\n",
      "Impact of row normalization: 0.0002 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Comparaison des stratégies d'imputation avec XGBoost ---\")\n",
    "\n",
    "# Liste des stratégies d'imputation, à la fois standards et avec features prétraitées\n",
    "imputation_strategies = [\"ffbf\"]\n",
    "imputation_results = []\n",
    "scalers = {\n",
    "    'Standard': StandardScaler(),\n",
    "    'MinMax': MinMaxScaler(),\n",
    "    'Robust': RobustScaler(),\n",
    "    'Quantile': QuantileTransformer(output_distribution='normal')\n",
    "}\n",
    "# Tester d'abord les datasets standards\n",
    "for strategy in imputation_strategies:\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"\\n Using {scaler} - Testing normalisation on {strategy} with precomputed features...\")\n",
    "        print(f\"\\nTesting {strategy} imputation...\")\n",
    "        try:\n",
    "            # Sans feature engineering (utilisant le dataset standard)\n",
    "            result_without_features = run_experiment(dataset_key=strategy, model_key=\"xgboost_baseline\", add_feat=False, scaler=scaler)\n",
    "            results_tracker = add_result(results_tracker, result_without_features)\n",
    "            imputation_results.append(result_without_features)\n",
    "            print(f\"Accuracy without feature engineering: {result_without_features['accuracy']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing standard {strategy} dataset: {e}\")\n",
    "\n",
    "# Ensuite tester les datasets prétraités avec features\n",
    "for strategy in imputation_strategies:\n",
    "    print(f\"\\nTesting {strategy} with precomputed features...\")\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"\\n Using {scaler} - Testing normalisation on {strategy} with precomputed features...\")\n",
    "        try:\n",
    "            # Construire le chemin d'accès aux datasets prétraités\n",
    "            preprocessed_key = f\"{strategy}_with_features\"\n",
    "            \n",
    "            # Vérifier si le dataset existe dans le registre\n",
    "            if preprocessed_key in DATASETS:\n",
    "                # Utiliser directement le dataset prétraité (pas besoin d'add_feat)\n",
    "                result_with_features = run_experiment(dataset_key=preprocessed_key, model_key=\"xgboost_baseline\", add_feat=False, scaler=scaler)\n",
    "                results_tracker = add_result(results_tracker, result_with_features)\n",
    "                imputation_results.append(result_with_features)\n",
    "                print(f\"Accuracy with precomputed features: {result_with_features['accuracy']:.4f}\")\n",
    "                \n",
    "                # Si nous avons les deux résultats, calculer l'amélioration\n",
    "                standard_result = next((r for r in imputation_results if r['dataset'] == strategy), None)\n",
    "                if standard_result:\n",
    "                    improvement = result_with_features['accuracy'] - standard_result['accuracy']\n",
    "                    print(f\"Improvement from feature engineering: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "            else:\n",
    "                print(f\"Preprocessed dataset '{preprocessed_key}' not found in registry.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing preprocessed {strategy} dataset: {e}\")\n",
    "\n",
    "# Tester avec normalisation par ligne (optionnel)\n",
    "print(\"\\n--- Testing with row normalization ---\")\n",
    "for strategy in imputation_strategies:\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        print(f\"\\n Using {scaler} - Testing normalisation on {strategy} with precomputed features...\")\n",
    "        preprocessed_key = f\"{strategy}_with_features\"\n",
    "        if preprocessed_key in DATASETS:\n",
    "            try:\n",
    "                print(f\"\\nTesting normalisation on {strategy} with precomputed features...\")\n",
    "                result_normalized = run_experiment(\n",
    "                    dataset_key=preprocessed_key, \n",
    "                    model_key=\"xgboost_baseline\", \n",
    "                    add_feat=False,\n",
    "                    normalize_by_row=True,  # Activer la normalisation par ligne\n",
    "                    scaler=scaler\n",
    "                )\n",
    "                results_tracker = add_result(results_tracker, result_normalized)\n",
    "                imputation_results.append(result_normalized)\n",
    "                print(f\"{strategy} with row normalization: Accuracy = {result_normalized['accuracy']:.4f}\")\n",
    "                \n",
    "                # Comparer avec version sans normalisation\n",
    "                non_normalized = next((r for r in imputation_results if r['dataset'] == preprocessed_key and not r.get('normalize_by_row', False)), None)\n",
    "                if non_normalized:\n",
    "                    diff = result_normalized['accuracy'] - non_normalized['accuracy']\n",
    "                    print(f\"Impact of row normalization: {diff:.4f} ({diff*100:.2f}%)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with row normalization on {strategy}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503\n"
     ]
    }
   ],
   "source": [
    "print(x_train_ffbf[\"day\"].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On remaruqe que malgré la baisse de performance avec l'utilisation des features, on a une augmentation de l'accuracy avec le standard scaler et row normalisation. On va essayer d'analyser les données avec les lignes normalisées puis réduire les dimensions des features en retirant les features non pertinentes puis potentiellement en rajouter de nouvelles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_normalized_dataset(df, feature_groups=None):\n",
    "    \"\"\"\n",
    "    Analyze a normalized dataset, focusing on different groups of features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to analyze\n",
    "    feature_groups : dict, optional\n",
    "        Dictionary mapping feature group names to lists of columns to analyze\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    if feature_groups is None:\n",
    "        # Auto-detect feature groups\n",
    "        feature_groups = {\n",
    "            'rendements': [col for col in df.columns if col.startswith('r') and col[1:].isdigit()],\n",
    "            'basic_stats': [col for col in df.columns if col.startswith('r_') and not any(x in col for x in ['roll', 'momentum'])],\n",
    "            'roll_features': [col for col in df.columns if 'roll' in col],\n",
    "            'momentum_features': [col for col in df.columns if 'momentum' in col]\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic statistics for each feature group\n",
    "    stats = {}\n",
    "    for group_name, columns in feature_groups.items():\n",
    "        if not columns:\n",
    "            continue\n",
    "            \n",
    "        group_df = df[columns]\n",
    "        stats[group_name] = {\n",
    "            'mean': group_df.mean().describe(),\n",
    "            'std': group_df.std().describe(),\n",
    "            'missing': group_df.isna().sum().sum(),\n",
    "            'n_features': len(columns)\n",
    "        }\n",
    "    \n",
    "    results['stats'] = stats\n",
    "    \n",
    "    # 2. Correlation with target (if available)\n",
    "    if 'reod' in df.columns:\n",
    "        target_correlations = {}\n",
    "        for group_name, columns in feature_groups.items():\n",
    "            correlations = df[columns].corrwith(df['reod'])\n",
    "            target_correlations[group_name] = {\n",
    "                'strongest_positive': correlations.nlargest(3),\n",
    "                'strongest_negative': correlations.nsmallest(3),\n",
    "                'mean_abs_corr': correlations.abs().mean()\n",
    "            }\n",
    "        \n",
    "        results['target_correlations'] = target_correlations\n",
    "    \n",
    "    # 3. Plot distribution of feature values for each group\n",
    "    fig_dict = {}\n",
    "    for group_name, columns in feature_groups.items():\n",
    "        if not columns:\n",
    "            continue\n",
    "            \n",
    "        # Take a sample of columns if there are too many\n",
    "        sample_cols = columns[:5] if len(columns) > 5 else columns\n",
    "        \n",
    "        fig, axes = plt.subplots(len(sample_cols), 1, figsize=(12, 3*len(sample_cols)))\n",
    "        fig.suptitle(f'Distribution of {group_name} features')\n",
    "        \n",
    "        if len(sample_cols) == 1:\n",
    "            axes = [axes]  # Make it iterable when there's only one subplot\n",
    "            \n",
    "        for i, col in enumerate(sample_cols):\n",
    "            sns.histplot(df[col].dropna(), kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'{col} (mean={df[col].mean():.2f}, std={df[col].std():.2f})')\n",
    "            axes[i].axvline(df[col].mean(), color='r', linestyle='--')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        fig_dict[group_name] = fig\n",
    "    \n",
    "    results['figures'] = fig_dict\n",
    "    \n",
    "    # 4. Feature correlations within groups\n",
    "    correlation_matrices = {}\n",
    "    for group_name, columns in feature_groups.items():\n",
    "        if len(columns) < 2:  # Need at least 2 features for correlation\n",
    "            continue\n",
    "            \n",
    "        # Take a sample if there are too many columns\n",
    "        sample_cols = columns[:10] if len(columns) > 10 else columns\n",
    "        corr_matrix = df[sample_cols].corr()\n",
    "        correlation_matrices[group_name] = corr_matrix\n",
    "        \n",
    "        # Plot correlation matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', center=0,\n",
    "                   annot=True if len(sample_cols) <= 10 else False, \n",
    "                   fmt='.2f', square=True)\n",
    "        plt.title(f'Correlation Matrix: {group_name}')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "    results['correlation_matrices'] = correlation_matrices\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.experiment_runner import normalize_rendements_by_row\n",
    "#Cette fonction est utilisée automatiquement dans experiment_runner \n",
    "x_train_ffbf_with_feature_normalized = normalize_rendements_by_row(x_train_ffbf_with_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_results = analyze_normalized_dataset(x_train_ffbf_with_feature_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_important_features(df, target_col='reod', threshold=0.05):\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Target column '{target_col}' not found in DataFrame\")\n",
    "        return []\n",
    "    \n",
    "    # Calculate correlations with target\n",
    "    feature_cols = [col for col in df.columns if col != target_col and col not in ['ID']]\n",
    "    correlations = df[feature_cols].corrwith(df[target_col])\n",
    "    \n",
    "    # Filter features based on threshold\n",
    "    important_features = correlations[correlations.abs() > threshold].sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"Found {len(important_features)} features with correlation > {threshold}\")\n",
    "    print(\"\\nTop positively correlated features:\")\n",
    "    print(important_features.head(10))\n",
    "    print(\"\\nTop negatively correlated features:\")\n",
    "    print(important_features.tail(10))\n",
    "    \n",
    "    return important_features.index.tolist()\n",
    "\n",
    "def compare_normalization_impact(original_df, normalized_df, sample_cols=None):\n",
    "\n",
    "    # Get common columns between both DataFrames\n",
    "    common_cols = [col for col in original_df.columns if col in normalized_df.columns]\n",
    "    \n",
    "    # Select sample columns if not provided\n",
    "    if sample_cols is None:\n",
    "        # Focus on rendement columns\n",
    "        rendement_cols = [col for col in common_cols if col.startswith('r') and col[1:].isdigit()]\n",
    "        sample_cols = np.random.choice(rendement_cols, min(5, len(rendement_cols)), replace=False)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    for col in sample_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Original data distribution\n",
    "        sns.histplot(original_df[col].dropna(), kde=True, ax=axes[0])\n",
    "        axes[0].set_title(f'Original: {col}')\n",
    "        axes[0].axvline(original_df[col].mean(), color='r', linestyle='--', \n",
    "                         label=f'Mean: {original_df[col].mean():.2f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Normalized data distribution\n",
    "        sns.histplot(normalized_df[col].dropna(), kde=True, ax=axes[1])\n",
    "        axes[1].set_title(f'Normalized: {col}')\n",
    "        axes[1].axvline(normalized_df[col].mean(), color='r', linestyle='--',\n",
    "                        label=f'Mean: {normalized_df[col].mean():.2f}')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Compare correlation structures\n",
    "    print(\"Analyzing correlation structure changes...\")\n",
    "    \n",
    "    # Calculate correlation matrices\n",
    "    orig_corr = original_df[sample_cols].corr()\n",
    "    norm_corr = normalized_df[sample_cols].corr()\n",
    "    \n",
    "    # Plot them side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    sns.heatmap(orig_corr, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0])\n",
    "    axes[0].set_title('Original Correlation Matrix')\n",
    "    \n",
    "    sns.heatmap(norm_corr, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1])\n",
    "    axes[1].set_title('Normalized Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate the difference in correlation matrices\n",
    "    diff_corr = norm_corr - orig_corr\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(diff_corr, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Change in Correlation Matrix (Normalized - Original)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_distributions(df, sample_cols=5):\n",
    "    # Sélectionner des colonnes de rendement (r0, r1, etc.)\n",
    "    r_cols = [col for col in df.columns if col.startswith('r') and col[1:].isdigit()]\n",
    "    \n",
    "    # Prendre un échantillon si trop nombreuses\n",
    "    sample = r_cols[:sample_cols]\n",
    "    \n",
    "    # Créer une figure pour visualiser\n",
    "    fig, axes = plt.subplots(len(sample), 1, figsize=(12, 4*len(sample)))\n",
    "    \n",
    "    for i, col in enumerate(sample):\n",
    "        # Tracer l'histogramme avec courbe de densité\n",
    "        sns.histplot(df[col], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution de {col} après normalisation')\n",
    "        axes[i].axvline(0, color='r', linestyle='--')  # Ligne à la moyenne théorique (0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Afficher statistiques de base pour confirmer la normalisation\n",
    "    stats = df[r_cols].describe().T[['mean', 'std', 'min', 'max']]\n",
    "    return stats\n",
    "\n",
    "# Pour analyser les corrélations avec la cible\n",
    "def analyze_target_correlations(df, target_col='reod'):\n",
    "    # Exclure colonnes non-feature\n",
    "    non_features = ['ID', target_col]\n",
    "    features = [col for col in df.columns if col not in non_features]\n",
    "    \n",
    "    # Calculer les corrélations\n",
    "    correlations = df[features].corrwith(df[target_col])\n",
    "    \n",
    "    # Afficher les plus fortes (positives et négatives)\n",
    "    print(\"Top features par corrélation positive:\")\n",
    "    print(correlations.nlargest(10))\n",
    "    \n",
    "    print(\"\\nTop features par corrélation négative:\")\n",
    "    print(correlations.nsmallest(10))\n",
    "    \n",
    "    # Visualiser\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlations.abs().sort_values(ascending=False).head(20).plot(kind='bar')\n",
    "    plt.title('Top 20 features par corrélation absolue avec la cible')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = find_important_features(x_train_ffbf_with_feature_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_normalization_impact(x_train_ffbf_with_feature, x_train_ffbf_with_feature_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_features_with_various_methods(X, y, methods=['correlation', 'f_value', 'mutual_info', 'pca']):\n",
    "    \"\"\"\n",
    "    Select features using various methods and compare the results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix\n",
    "    y : pandas.Series\n",
    "        Target variable\n",
    "    methods : list\n",
    "        List of feature selection methods to use\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with selected features for each method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Method 1: Correlation with target\n",
    "    if 'correlation' in methods:\n",
    "        # Convert target to numeric if needed\n",
    "        y_numeric = pd.to_numeric(y, errors='coerce')\n",
    "        \n",
    "        # Calculate correlation with target\n",
    "        correlations = pd.Series(\n",
    "            [np.corrcoef(X[col].values, y_numeric.values)[0, 1] for col in feature_names],\n",
    "            index=feature_names\n",
    "        )\n",
    "        \n",
    "        # Get top features by absolute correlation\n",
    "        abs_corr = correlations.abs().sort_values(ascending=False)\n",
    "        top_corr_features = abs_corr.index.tolist()\n",
    "        \n",
    "        results['correlation'] = {\n",
    "            'features': top_corr_features,\n",
    "            'scores': abs_corr.values,\n",
    "            'top_positive': correlations.nlargest(10),\n",
    "            'top_negative': correlations.nsmallest(10)\n",
    "        }\n",
    "        \n",
    "        # Visualize correlation results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        correlations.abs().sort_values().tail(20).plot(kind='barh')\n",
    "        plt.title('Top 20 Features by Absolute Correlation with Target')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Method 2: ANOVA F-value\n",
    "    if 'f_value' in methods:\n",
    "        selector = SelectKBest(score_func=f_classif, k='all')\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get scores and p-values\n",
    "        f_scores = pd.Series(selector.scores_, index=feature_names)\n",
    "        p_values = pd.Series(selector.pvalues_, index=feature_names)\n",
    "        \n",
    "        # Get top features by F-score\n",
    "        top_f_features = f_scores.sort_values(ascending=False).index.tolist()\n",
    "        \n",
    "        results['f_value'] = {\n",
    "            'features': top_f_features,\n",
    "            'scores': f_scores.sort_values(ascending=False).values,\n",
    "            'p_values': p_values[top_f_features]\n",
    "        }\n",
    "        \n",
    "        # Visualize F-score results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        f_scores.sort_values().tail(20).plot(kind='barh')\n",
    "        plt.title('Top 20 Features by F-score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Method 3: Mutual Information\n",
    "    if 'mutual_info' in methods:\n",
    "        selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "        selector.fit(X, y)\n",
    "        \n",
    "        # Get scores\n",
    "        mi_scores = pd.Series(selector.scores_, index=feature_names)\n",
    "        \n",
    "        # Get top features by mutual information\n",
    "        top_mi_features = mi_scores.sort_values(ascending=False).index.tolist()\n",
    "        \n",
    "        results['mutual_info'] = {\n",
    "            'features': top_mi_features,\n",
    "            'scores': mi_scores.sort_values(ascending=False).values\n",
    "        }\n",
    "        \n",
    "        # Visualize mutual information results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        mi_scores.sort_values().tail(20).plot(kind='barh')\n",
    "        plt.title('Top 20 Features by Mutual Information')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Method 4: PCA\n",
    "    if 'pca' in methods:\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=min(X.shape[1], 100))  # Limit to 100 components max\n",
    "        pca.fit(X_scaled)\n",
    "        \n",
    "        # Get explained variance ratio\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        cumulative_variance = np.cumsum(explained_variance)\n",
    "        \n",
    "        # Find number of components needed for different variance thresholds\n",
    "        variance_thresholds = [0.7, 0.8, 0.9, 0.95]\n",
    "        components_needed = {}\n",
    "        \n",
    "        for threshold in variance_thresholds:\n",
    "            n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "            components_needed[threshold] = n_components\n",
    "        \n",
    "        results['pca'] = {\n",
    "            'explained_variance': explained_variance,\n",
    "            'cumulative_variance': cumulative_variance,\n",
    "            'components_needed': components_needed\n",
    "        }\n",
    "        \n",
    "        # Visualize PCA results\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-')\n",
    "        plt.title('Cumulative Explained Variance')\n",
    "        plt.xlabel('Number of Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        \n",
    "        # Add lines for thresholds\n",
    "        for threshold in variance_thresholds:\n",
    "            plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "            plt.text(len(cumulative_variance) * 0.8, threshold + 0.01, \n",
    "                    f\"{threshold*100}%: {components_needed[threshold]} components\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Compare feature selection methods\n",
    "    if len(methods) > 1:\n",
    "        common_methods = [m for m in methods if m != 'pca']\n",
    "        \n",
    "        if len(common_methods) > 1:\n",
    "            method_pairs = [(i, j) for i in range(len(common_methods)) for j in range(i+1, len(common_methods))]\n",
    "            \n",
    "            for i, j in method_pairs:\n",
    "                method1, method2 = common_methods[i], common_methods[j]\n",
    "                \n",
    "                # Get top features from each method\n",
    "                top_features1 = results[method1]['features'][:50]  # Top 50 features\n",
    "                top_features2 = results[method2]['features'][:50]\n",
    "                \n",
    "                # Find common features\n",
    "                common_features = set(top_features1).intersection(set(top_features2))\n",
    "                \n",
    "                print(f\"Overlap between {method1} and {method2} (top 50): {len(common_features)} features\")\n",
    "                print(f\"Common features: {sorted(list(common_features)[:10])}...\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Méthode 1: Sélection par corrélation avec la cible\n",
    "def select_by_correlation(X, y, top_n=None, plot=True):\n",
    "    # Convertir la cible en valeurs numériques si nécessaire\n",
    "    y_numeric = pd.to_numeric(y, errors='coerce')\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Calculer la corrélation de chaque feature avec la cible\n",
    "    correlations = pd.Series(\n",
    "        [np.corrcoef(X[col].values, y_numeric.values)[0, 1] for col in feature_names],\n",
    "        index=feature_names\n",
    "    )\n",
    "    \n",
    "    # Trier par corrélation absolue (positive ou négative)\n",
    "    abs_corr = correlations.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Sélectionner le nombre de features demandé\n",
    "    if top_n:\n",
    "        selected_features = abs_corr.head(top_n).index.tolist()\n",
    "    else:\n",
    "        selected_features = abs_corr.index.tolist()\n",
    "    \n",
    "    # Visualiser les résultats si demandé\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        abs_corr.head(20).plot(kind='barh')\n",
    "        plt.title('Top 20 features par corrélation absolue avec la cible')\n",
    "        plt.xlabel('Corrélation absolue')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Afficher les corrélations positives et négatives\n",
    "        print(\"Top corrélations positives:\")\n",
    "        print(correlations.nlargest(10))\n",
    "        print(\"\\nTop corrélations négatives:\")\n",
    "        print(correlations.nsmallest(10))\n",
    "    \n",
    "    return {\n",
    "        'selected_features': selected_features,\n",
    "        'correlations': correlations,\n",
    "        'abs_correlations': abs_corr\n",
    "    }\n",
    "\n",
    "# Méthode 2: Sélection par test ANOVA F-value\n",
    "def select_by_f_value(X, y, top_n=None, plot=True):\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Appliquer le test F de l'ANOVA\n",
    "    selector = SelectKBest(score_func=f_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Récupérer les scores et p-values\n",
    "    f_scores = pd.Series(selector.scores_, index=feature_names)\n",
    "    p_values = pd.Series(selector.pvalues_, index=feature_names)\n",
    "    \n",
    "    # Trier par score F décroissant\n",
    "    sorted_features = f_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Sélectionner le nombre de features demandé\n",
    "    if top_n:\n",
    "        selected_features = sorted_features.head(top_n).index.tolist()\n",
    "    else:\n",
    "        selected_features = sorted_features.index.tolist()\n",
    "    \n",
    "    # Visualiser les résultats si demandé\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sorted_features.head(20).plot(kind='barh')\n",
    "        plt.title('Top 20 features par F-score (ANOVA)')\n",
    "        plt.xlabel('F-score')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Afficher les p-values pour voir la significativité statistique\n",
    "        print(\"Top features par F-score avec leurs p-values:\")\n",
    "        for feature in sorted_features.head(10).index:\n",
    "            print(f\"{feature}: F-score = {f_scores[feature]:.2f}, p-value = {p_values[feature]:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        'selected_features': selected_features,\n",
    "        'f_scores': f_scores,\n",
    "        'p_values': p_values\n",
    "    }\n",
    "\n",
    "# Méthode 3: Sélection par information mutuelle\n",
    "def select_by_mutual_info(X, y, top_n=None, plot=True):\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    # Appliquer la sélection par information mutuelle\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    # Récupérer les scores\n",
    "    mi_scores = pd.Series(selector.scores_, index=feature_names)\n",
    "    \n",
    "    # Trier par score décroissant\n",
    "    sorted_features = mi_scores.sort_values(ascending=False)\n",
    "    \n",
    "    # Sélectionner le nombre de features demandé\n",
    "    if top_n:\n",
    "        selected_features = sorted_features.head(top_n).index.tolist()\n",
    "    else:\n",
    "        selected_features = sorted_features.index.tolist()\n",
    "    \n",
    "    # Visualiser les résultats si demandé\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sorted_features.head(20).plot(kind='barh')\n",
    "        plt.title('Top 20 features par information mutuelle')\n",
    "        plt.xlabel('Score d\\'information mutuelle')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Top features par information mutuelle:\")\n",
    "        print(sorted_features.head(10))\n",
    "    \n",
    "    return {\n",
    "        'selected_features': selected_features,\n",
    "        'mi_scores': mi_scores\n",
    "    }\n",
    "\n",
    "# Fonction pour comparer les résultats des différentes méthodes\n",
    "def compare_feature_selection_methods(results_dict, top_n=20):\n",
    "    methods = list(results_dict.keys())\n",
    "    \n",
    "    if len(methods) < 2:\n",
    "        print(\"Besoin d'au moins deux méthodes pour faire une comparaison\")\n",
    "        return\n",
    "    \n",
    "    # Créer un DataFrame pour comparer les top features de chaque méthode\n",
    "    comparison = pd.DataFrame()\n",
    "    \n",
    "    for method in methods:\n",
    "        # Prendre les top_n features de chaque méthode\n",
    "        features = results_dict[method]['selected_features'][:top_n]\n",
    "        comparison[method] = pd.Series(features)\n",
    "    \n",
    "    print(f\"Comparaison des top {top_n} features par méthode:\")\n",
    "    print(comparison)\n",
    "    \n",
    "    # Trouver les features communes entre toutes les méthodes\n",
    "    common_features = set(results_dict[methods[0]]['selected_features'][:top_n])\n",
    "    for method in methods[1:]:\n",
    "        common_features &= set(results_dict[method]['selected_features'][:top_n])\n",
    "    \n",
    "    print(f\"\\nFeatures communes à toutes les méthodes: {len(common_features)}\")\n",
    "    if common_features:\n",
    "        print(sorted(list(common_features)))\n",
    "    \n",
    "    # Visualiser le recouvrement entre les méthodes avec un diagramme de Venn (si possible)\n",
    "    try:\n",
    "        from matplotlib_venn import venn2, venn3\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        if len(methods) == 2:\n",
    "            venn2([\n",
    "                set(results_dict[methods[0]]['selected_features'][:top_n]),\n",
    "                set(results_dict[methods[1]]['selected_features'][:top_n])\n",
    "            ], set_labels=methods)\n",
    "        elif len(methods) == 3:\n",
    "            venn3([\n",
    "                set(results_dict[methods[0]]['selected_features'][:top_n]),\n",
    "                set(results_dict[methods[1]]['selected_features'][:top_n]),\n",
    "                set(results_dict[methods[2]]['selected_features'][:top_n])\n",
    "            ], set_labels=methods)\n",
    "        else:\n",
    "            print(\"Diagramme de Venn limité à 2 ou 3 ensembles\")\n",
    "        \n",
    "        plt.title(f\"Recouvrement des top {top_n} features par méthode\")\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Package matplotlib_venn non disponible pour le diagramme de Venn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X = x_train_ffbf_with_feature_normalized.drop(['ID', 'reod'], axis=1, errors='ignore')\n",
    "y = y_train_ffbg_with_features\n",
    "\n",
    "# Appliquer chaque méthode de sélection\n",
    "corr_results = select_by_correlation(X, y, top_n=50)\n",
    "f_results = select_by_f_value(X, y, top_n=50)\n",
    "mi_results = select_by_mutual_info(X, y, top_n=50)\n",
    "\n",
    "# Comparer les résultats\n",
    "all_results = {\n",
    "    'correlation': corr_results,\n",
    "    'f_value': f_results,\n",
    "    'mutual_info': mi_results\n",
    "}\n",
    "compare_feature_selection_methods(all_results, top_n=20)\n",
    "\n",
    "# Pour utiliser les features sélectionnées, vous pourriez choisir:\n",
    "# 1. Soit les features communes à toutes les méthodes\n",
    "common_features = set(corr_results['selected_features'][:20]) & \\\n",
    "                 set(f_results['selected_features'][:20]) & \\\n",
    "                 set(mi_results['selected_features'][:20])\n",
    "                 \n",
    "# 2. Soit les features d'une méthode spécifique que vous préférez\n",
    "top_features = corr_results['selected_features'][:30]  # Par exemple\n",
    "\n",
    "# 3. Soit une union des top N features de chaque méthode\n",
    "union_features = set()\n",
    "for method_results in all_results.values():\n",
    "    union_features.update(method_results['selected_features'][:10])\n",
    "\n",
    "print(f\"Nombre de features uniques dans l'union des top 10 de chaque méthode: {len(union_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_feature_count(X, y, model_factory, feature_ranking, \n",
    "                          n_features_range=range(5, 101, 5),\n",
    "                          scale_method='standard',\n",
    "                          cv=5):\n",
    "    \"\"\"\n",
    "    Trouve le nombre optimal de features à utiliser pour maximiser la performance du modèle.\n",
    "    \n",
    "    Arguments:\n",
    "        X: DataFrame contenant toutes les features\n",
    "        y: Series contenant la variable cible\n",
    "        model_factory: Fonction qui crée une nouvelle instance du modèle\n",
    "        feature_ranking: Liste des features ordonnées par importance décroissante\n",
    "        n_features_range: Plage du nombre de features à tester\n",
    "        scale_method: Méthode de normalisation ('standard', 'robust', 'quantile', None)\n",
    "        cv: Nombre de folds pour la validation croisée\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Sélectionner le bon scaler selon la méthode demandée\n",
    "    if scale_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scale_method == 'robust':\n",
    "        scaler = RobustScaler()\n",
    "    elif scale_method == 'quantile':\n",
    "        scaler = QuantileTransformer(output_distribution='normal')\n",
    "    else:\n",
    "        scaler = None\n",
    "    \n",
    "    # Stocker les résultats pour chaque nombre de features\n",
    "    results = []\n",
    "    \n",
    "    # Tester différents nombres de features\n",
    "    for n_features in tqdm(n_features_range):\n",
    "        # Sélectionner les top n features\n",
    "        selected_features = feature_ranking[:n_features]\n",
    "        X_selected = X[selected_features]\n",
    "        \n",
    "        # Créer le pipeline avec ou sans scaler\n",
    "        if scaler:\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('model', model_factory())\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = model_factory()\n",
    "        \n",
    "        # Évaluer la performance avec validation croisée\n",
    "        cv_scores = cross_val_score(pipeline, X_selected, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        # Stocker le résultat\n",
    "        results.append({\n",
    "            'n_features': n_features,\n",
    "            'mean_cv_score': cv_scores.mean(),\n",
    "            'std_cv_score': cv_scores.std(),\n",
    "            'features': selected_features\n",
    "        })\n",
    "    \n",
    "    # Convertir en DataFrame pour faciliter l'analyse\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Trouver le nombre optimal de features\n",
    "    best_result = results_df.loc[results_df['mean_cv_score'].idxmax()]\n",
    "    optimal_n_features = int(best_result['n_features'])\n",
    "    best_score = best_result['mean_cv_score']\n",
    "    \n",
    "    print(f\"Nombre optimal de features: {optimal_n_features}\")\n",
    "    print(f\"Score de validation croisée: {best_score:.4f} ± {best_result['std_cv_score']:.4f}\")\n",
    "    \n",
    "    # Visualiser les résultats\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(results_df['n_features'], results_df['mean_cv_score'], \n",
    "                 yerr=results_df['std_cv_score'], fmt='o-')\n",
    "    plt.axvline(x=optimal_n_features, color='r', linestyle='--', \n",
    "                label=f'Optimal: {optimal_n_features} features')\n",
    "    plt.xlabel('Nombre de features')\n",
    "    plt.ylabel('Score de validation croisée (accuracy)')\n",
    "    plt.title('Impact du nombre de features sur la performance du modèle')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'optimal_n_features': optimal_n_features,\n",
    "        'best_score': best_score,\n",
    "        'optimal_features': best_result['features'],\n",
    "        'results': results_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# D'abord, obtenir un classement des features (par exemple via corrélation)\n",
    "corr_results = select_by_correlation(X, y)\n",
    "feature_ranking = corr_results['selected_features']\n",
    "\n",
    "# Définir une factory function qui crée une nouvelle instance du modèle\n",
    "def model_factory():\n",
    "    return XGBClassifier(\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=3,\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Trouver le nombre optimal de features\n",
    "optimization_results = optimize_feature_count(\n",
    "    X, y, \n",
    "    model_factory=model_factory,\n",
    "    feature_ranking=feature_ranking,\n",
    "    n_features_range=range(5, 101, 5)  # Tester de 5 à 100 features par pas de 5\n",
    ")\n",
    "\n",
    "# Utiliser les features optimales pour la modélisation finale\n",
    "optimal_features = optimization_results['optimal_features']\n",
    "print(f\"Features optimales: {optimal_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
