{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdd4658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# --- Markdown Cell ---\n",
      "# Prédiction de la direction des prix sur les marchés financiers\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "## 1. Introduction et présentation du problème\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 1.1 Contexte et enjeux\n",
      "# \n",
      "# Dans le cadre de ce challenge (issu du Collège de France ou du Lab Banque de France), nous disposons de données de marché avec pour objectif de prédire la direction du prix (baisse, stable, hausse) en fin de journée, à partir des rendements du matin.\n",
      "# \n",
      "# Ce marché américain étant particulièrement liquide, l'enjeu est de pouvoir estimer la tendance entre 14h et 16h pour prendre des décisions d'investissement ou d'arbitrage.\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 1.2 Description des données\n",
      "# \n",
      "# - **Index des données**\n",
      "#   - Chaque ligne représente un jour donné et une action donnée (identifiants : `day` et `equity`).\n",
      "#   - Les colonnes `r0` à `r52` correspondent aux rendements (en points de base) toutes les 5 minutes entre 9h30 et 14h.\n",
      "# \n",
      "# - **Variables explicatives**\n",
      "#   - Les 53 rendements (`r0, r1, …, r52`), éventuellement d'autres features dérivées.\n",
      "# \n",
      "# - **Variable cible**\n",
      "#   - `reod` {-1, 0, 1} indiquant la tendance de l'actif entre 14h et 16h (baisse, stable ou hausse).\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 1.3 Problématique et défis\n",
      "# \n",
      "# Notre mission est de prédire la classe de rendement (`reod`) en fin de journée, à partir des données matinales. Les défis principaux sont :\n",
      "# \n",
      "# - La **taille importante** du dataset (plusieurs centaines de milliers de lignes).\n",
      "# - La **présence de valeurs manquantes** (`NaN`).\n",
      "# - L'**absence de jours/actions communs** entre le jeu d'entraînement et le jeu de test, ce qui complique l'utilisation directe de `equity` ou `day` en tant que features.\n",
      "# - Le **risque de surcoût mémoire** et de temps de calcul avec certains modèles comme les Random Forest.\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "## 2. Exploration des données (EDA)\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### 2.1 Aperçu des données et analyse des valeurs manquantes\n",
      "# \n",
      "# Nous commençons par explorer les données d'entraînement pour comprendre leur structure et la distribution des valeurs manquantes.\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### Importation des librairies\n",
      "# --- Code Cell ---\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import time\n",
      "from datetime import datetime\n",
      "\n",
      "from sklearn.impute import SimpleImputer\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
      "\n",
      "# Configuration de l'affichage\n",
      "plt.style.use('seaborn-v0_8-whitegrid')\n",
      "sns.set_context(\"notebook\", font_scale=1.2)\n",
      "plt.rcParams['figure.figsize'] = (12, 8)\n",
      "# --- Markdown Cell ---\n",
      "### Importation des données\n",
      "# --- Code Cell ---\n",
      "\n",
      "# Importation des données d'entraînement\n",
      "X = pd.read_csv('input_training.csv')\n",
      "X.sort_values(by=\"ID\",inplace=True)\n",
      "y = pd.read_csv('output/output_training_gmEd6Zt.csv')\n",
      "\n",
      "# Importation des données de test\n",
      "data_test = pd.read_csv('input_test.csv')\n",
      "data_test.sort_values(by=\"ID\",inplace=True)\n",
      "y_test = pd.read_csv(\"output/output_test_random.csv\")\n",
      "\n",
      "# Fusion des données et labels\n",
      "X_train = pd.merge(X, y, on=\"ID\").copy()\n",
      "data_test.sort_values(by=\"ID\", inplace=True)\n",
      "X_test = pd.merge(data_test, y_test, on=\"ID\").copy()\n",
      "\n",
      "# Aperçu des données\n",
      "print(\"\\n--- Aperçu des données d'entraînement ---\")\n",
      "print(f\"Nombre de lignes train: {X_train.shape[0]}, Nombre de colonnes: {X_train.shape[1]}\")\n",
      "print(f\"Nombre de lignes test: {X_test.shape[0]}, Nombre de colonnes: {X_test.shape[1]}\")\n",
      "print(\"\\nPremières lignes:\")\n",
      "display(X_train.head())\n",
      "# --- Markdown Cell ---\n",
      "### Analyse des valeurs manquantes \n",
      "# --- Code Cell ---\n",
      "col_rendements = [col for col in X_train.columns if col.startswith(\"r\")]\n",
      "# Analyse du nombre de valeurs manquantes par colonne\n",
      "plt.figure(figsize=(16, 4))\n",
      "\n",
      "missing_values_count = X_train[col_rendements].isna().sum()\n",
      "missing_values_percent = (missing_values_count / len(X_train)) * 100\n",
      "missing_df = pd.DataFrame({\n",
      "    'Nombre de NaN': missing_values_count,\n",
      "    'Pourcentage (%)': missing_values_percent\n",
      "}).sort_values('Nombre de NaN', ascending=False)\n",
      "\n",
      "ax = missing_df['Pourcentage (%)'].plot.bar()\n",
      "plt.title('Pourcentage de valeurs manquantes par colonne de rendement')\n",
      "plt.ylabel('Pourcentage de valeurs manquantes (%)')\n",
      "plt.xlabel('Colonne')\n",
      "plt.xticks(rotation=90)\n",
      "\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Code Cell ---\n",
      "# Analyse du nombre de valeurs manquantes par ligne\n",
      "NaN_analysis = pd.DataFrame(index=X_train.index, columns=[\"NaN_count\", \"NaN_percent\"])\n",
      "NaN_analysis[\"NaN_count\"] = X_train[col_rendements].isna().sum(axis=1)\n",
      "nombre_colonnes_rend = len(col_rendements)\n",
      "NaN_analysis[\"NaN_percent\"] = (NaN_analysis[\"NaN_count\"] / nombre_colonnes_rend) * 100\n",
      "\n",
      "# Compter le nombre de lignes avec des NaN\n",
      "nbr_row_na = X_train.isna().any(axis=1).sum()\n",
      "\n",
      "print(f\"Nombre de lignes totales du dataset: {len(X_train)}\")\n",
      "print(f\"Nombre de lignes contenant au moins un NaN: {nbr_row_na}, soit {(nbr_row_na/len(X_train)*100):.2f}%\")\n",
      "print(f\"Nombre de lignes avec plus de 30% de NaN: {len(NaN_analysis[NaN_analysis['NaN_percent']>30])}, soit {len(NaN_analysis[NaN_analysis['NaN_percent']>30])/len(X_train)*100:.2f}%\")\n",
      "print(f\"Nombre de lignes avec plus de 50% de NaN: {len(NaN_analysis[NaN_analysis['NaN_percent']>50])}, soit {len(NaN_analysis[NaN_analysis['NaN_percent']>50])/len(X_train)*100:.2f}%\")\n",
      "\n",
      "plt.figure(figsize=(12, 6))\n",
      "sns.histplot(NaN_analysis[\"NaN_percent\"], bins=50, kde=True)\n",
      "plt.title('Distribution du pourcentage de valeurs manquantes par ligne')\n",
      "plt.xlabel('Pourcentage de valeurs manquantes (%)')\n",
      "plt.ylabel('Nombre de lignes')\n",
      "plt.axvline(x=30, color='r', linestyle='--', label='Seuil de 30%')\n",
      "plt.axvline(x=50, color='g', linestyle='--', label='Seuil de 50%')\n",
      "plt.legend()\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Code Cell ---\n",
      "# Distribution spatiale des NaN (heatmap)\n",
      "plt.figure(figsize=(20, 10))\n",
      "sample_size = min(1000, len(X_train))  # Limiter à 1000 lignes pour la lisibilité\n",
      "sample_indices = np.random.choice(range(len(X_train)), sample_size, replace=False)\n",
      "sample_data = X_train.iloc[sample_indices][col_rendements].isna()\n",
      "sns.heatmap(sample_data, cbar=False, cmap='viridis')\n",
      "plt.title('Distribution spatiale des valeurs manquantes (échantillon aléatoire)')\n",
      "plt.xlabel('Colonne de rendement')\n",
      "plt.ylabel('Ligne (échantillon)')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 2.2 Distribution des rendements\n",
      "# \n",
      "# Analysons la distribution des rendements pour identifier d'éventuelles caractéristiques ou anomalies.\n",
      "\n",
      "# --- Code Cell ---\n",
      "col_rendements.remove(\"reod\")\n",
      "# --- Code Cell ---\n",
      "plt.figure(figsize=(15, 50))\n",
      "\n",
      "for i, col in enumerate(col_rendements):\n",
      "    plt.subplot(int(len(col_rendements)/3) + 1, 3, i+1)\n",
      "    sns.histplot(X_train[col].dropna(), kde=True, bins=50)\n",
      "    plt.title(f'Distribution de {col}')\n",
      "    plt.xlabel('Rendement (points de base)')\n",
      "    plt.ylabel('Fréquence')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Code Cell ---\n",
      "rendements_stats = X_train[col_rendements].describe().T\n",
      "rendements_stats['missing_percent'] = X_train[col_rendements].isna().mean() * 100\n",
      "rendements_stats = rendements_stats.sort_values('max', ascending=False)\n",
      "rendements_stats.head(10)\n",
      "# --- Code Cell ---\n",
      "plt.figure(figsize=(15, 6))\n",
      "plt.subplot(1, 2, 1)\n",
      "plt.scatter(rendements_stats.index, rendements_stats['mean'], alpha=0.6)\n",
      "plt.title('Moyenne des rendements par colonne')\n",
      "plt.xticks(rotation=90)\n",
      "plt.ylabel('Moyenne')\n",
      "\n",
      "plt.subplot(1, 2, 2)\n",
      "plt.scatter(rendements_stats.index, rendements_stats['std'], alpha=0.6, color='orange')\n",
      "plt.title('Écart-type des rendements par colonne')\n",
      "plt.xticks(rotation=90)\n",
      "plt.ylabel('Écart-type')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 2.3 Analyse de la variable cible\n",
      "# \n",
      "# Examinons la distribution de notre variable cible `reod` pour vérifier l'équilibre entre les classes.\n",
      "\n",
      "# --- Code Cell ---\n",
      "# Distribution des classes\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "class_counts = X_train['reod'].value_counts().sort_index()\n",
      "ax = sns.barplot(x=class_counts.index, y=class_counts.values)\n",
      "\n",
      "plt.title('Distribution des classes (reod)')\n",
      "plt.xlabel('Classe (-1: Baisse, 0: Stable, 1: Hausse)')\n",
      "plt.ylabel('Nombre d\\'observations')\n",
      "\n",
      "total = len(X_train)\n",
      "for i, v in enumerate(class_counts):\n",
      "    ax.text(i, v + 5000, f'{v} ({v/total*100:.1f}%)', ha='center')\n",
      "    \n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# --- Code Cell ---\n",
      "# Analyse des rendements par classe\n",
      "plt.figure(figsize=(20,60))\n",
      "\n",
      "for i, col in enumerate(col_rendements):\n",
      "    \n",
      "    plt.subplot(int(len(col_rendements)/3) + 1, 3, i+1)\n",
      "    for cls in sorted(X_train['reod'].unique()):\n",
      "        subset = X_train[X_train['reod'] == cls][col].dropna()\n",
      "        sns.kdeplot(subset, label=f'Classe {cls}')\n",
      "    plt.title(f'Distribution de {col} par classe')\n",
      "    plt.xlabel('Rendement (points de base)')\n",
      "    plt.ylabel('Densité')\n",
      "    plt.legend()\n",
      "    \n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Code Cell ---\n",
      "# Corrélation entre les rendements et la variable cible\n",
      "correlations = X_train[col_rendements + ['reod']].corr()['reod'].drop('reod').sort_values(ascending=False)\n",
      "\n",
      "plt.figure(figsize=(12, 6))\n",
      "sns.barplot(x=correlations.index, y=correlations.values)\n",
      "plt.title('Corrélation entre les rendements et la variable cible (reod)')\n",
      "plt.xticks(rotation=90)\n",
      "plt.xlabel('Colonne de rendement')\n",
      "plt.ylabel('Coefficient de corrélation')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "print(\"Top 5 des rendements positivement corrélés avec reod:\")\n",
      "display(correlations.head())\n",
      "\n",
      "print(\"\\nTop 5 des rendements négativement corrélés avec reod:\")\n",
      "display(correlations.tail())\n",
      "# --- Markdown Cell ---\n",
      "## 3. Stratégies d'imputation\n",
      "# \n",
      "# Face au défi des valeurs manquantes, nous testons plusieurs stratégies d'imputation pour compléter les données.\n",
      "# \n",
      "# - 1 :  Forward-fill puis backward-fill (FFBF)\n",
      "# \n",
      "# Cette stratégie propage d'abord les dernières valeurs connues vers l'avant, puis remplit les valeurs restantes en propageant depuis la fin.|\n",
      "# \n",
      "# - 2 : Backward-fill puis forward-fill (BFFF)\n",
      "# \n",
      "# Cette stratégie inverse commence par propager depuis la fin, puis remplit les valeurs restantes en propageant depuis le début.\n",
      "# \n",
      "# \n",
      "# - 3 : Interpolation linéaire\n",
      "# \n",
      "# Cette méthode crée une ligne droite entre les valeurs connues pour estimer les valeurs manquantes.\n",
      "# \n",
      "# - 4 : Imputation par K plus proches voisins (KNN)\n",
      "# \n",
      "# Cette méthode utilise les K observations les plus similaires pour estimer les valeurs manquantes.\n",
      "# \n",
      "# - 5 : MICE \n",
      "\n",
      "# --- Code Cell ---\n",
      "from utils.data_registry import DATASETS\n",
      "from utils.features import add_features\n",
      "from utils.benchmarks import get_models\n",
      "from utils.experiment_runner import run_experiment, display_experiment_result, add_result\n",
      "from utils.load_data import load_datasets\n",
      "from utils.graphic import analyze_distributions, compare_column_stats, analyze_normalization\n",
      "from utils.variance_analysis import perform_pca_analysis, perform_tsne_analysis, analyze_correlations, evaluate_feature_sets, analyze_feature_importance, select_best_features, summarize_feature_analysis\n",
      "from utils.preprocess_data import precompute_datasets_with_features\n",
      "# Create empty results tracker\n",
      "results_tracker = pd.DataFrame(columns=[\n",
      "    \"dataset\", \"dataset_description\", \"model\", \"model_description\", \n",
      "    \"features_added\", \"feature_sets\", \"accuracy\", \"precision_weighted\", \n",
      "    \"recall_weighted\", \"f1_weighted\"\n",
      "])\n",
      "\n",
      "# Charger les datasets\n",
      "imputed_datasets = load_datasets()\n",
      "# --- Code Cell ---\n",
      "#Calcule l'ensemble des features sur les datasets existants, à ne lancer que s'il n'existent pas \n",
      "# precompute_datasets_with_features()\n",
      "# --- Code Cell ---\n",
      "analyze_distributions(imputed_datasets)\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 3.2 Comparaison des stratégies d'imputation\n",
      "# \n",
      "# Nous comparons les distributions après imputation pour évaluer l'impact de chaque méthode.\n",
      "\n",
      "# --- Code Cell ---\n",
      "stats_results = compare_column_stats(imputed_datasets)\n",
      "# --- Markdown Cell ---\n",
      "### 3.3 Différentes méthodes de normalisation : \n",
      "# --- Code Cell ---\n",
      "# Analyser l'impact des différentes méthodes de normalisation\n",
      "normalization_results = analyze_normalization(imputed_datasets)\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "## 4. Modèles de référence (benchmarks)\n",
      "# \n",
      "# Nous établissons des modèles de base pour servir de références aux futures améliorations.\n",
      "\n",
      "# --- Code Cell ---\n",
      "print(\"\\nDatasets disponibles:\")\n",
      "for key, info in DATASETS.items():\n",
      "    print(f\"- {key}: {info['description']}\")\n",
      "\n",
      "print(\"\\nModèles disponibles:\")\n",
      "models = get_models()\n",
      "for key, info in models.items():\n",
      "    if 'description' in info:\n",
      "        print(f\"- {key}: {info['description']}\")\n",
      "    else:\n",
      "        print(f\"- {key}\")\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 4.1 Configuration des expériences\n",
      "\n",
      "# --- Code Cell ---\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 4.2 XGBoost baseline\n",
      "# \n",
      "# Le gradient boosting est souvent efficace pour les données tabulaires et gère bien les valeurs manquantes.\n",
      "\n",
      "# --- Code Cell ---\n",
      "print(\"\\n--- Baseline XGBoost sur données brutes ---\")\n",
      "baseline_result = run_experiment(dataset_key=\"raw\", model_key=\"xgboost_baseline\", add_feat=False)\n",
      "results_tracker = add_result(results_tracker, baseline_result)\n",
      "print(\"\\nRésultats détaillés de l'expérience baseline:\")\n",
      "display_experiment_result(baseline_result)\n",
      "# --- Code Cell ---\n",
      "print(\"\\n--- Comparaison des stratégies d'imputation avec XGBoost ---\")\n",
      "\n",
      "# Liste des stratégies d'imputation, à la fois standards et avec features prétraitées\n",
      "imputation_strategies = [\"raw\", \"ffbf\", \"bfff\", \"interp\", \"knn\", \"mice\"]\n",
      "imputation_results = []\n",
      "\n",
      "# Tester d'abord les datasets standards\n",
      "for strategy in imputation_strategies:\n",
      "    print(f\"\\nTesting {strategy} imputation...\")\n",
      "    try:\n",
      "        # Sans feature engineering (utilisant le dataset standard)\n",
      "        result_without_features = run_experiment(dataset_key=strategy, model_key=\"xgboost_baseline\", add_feat=False)\n",
      "        results_tracker = add_result(results_tracker, result_without_features)\n",
      "        imputation_results.append(result_without_features)\n",
      "        print(f\"Accuracy without feature engineering: {result_without_features['accuracy']:.4f}\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error processing standard {strategy} dataset: {e}\")\n",
      "\n",
      "# Ensuite tester les datasets prétraités avec features\n",
      "for strategy in imputation_strategies:\n",
      "    print(f\"\\nTesting {strategy} with precomputed features...\")\n",
      "    try:\n",
      "        # Construire le chemin d'accès aux datasets prétraités\n",
      "        preprocessed_key = f\"{strategy}_with_features\"\n",
      "        \n",
      "        # Vérifier si le dataset existe dans le registre\n",
      "        if preprocessed_key in DATASETS:\n",
      "            # Utiliser directement le dataset prétraité (pas besoin d'add_feat)\n",
      "            result_with_features = run_experiment(dataset_key=preprocessed_key, model_key=\"xgboost_baseline\", add_feat=False)\n",
      "            results_tracker = add_result(results_tracker, result_with_features)\n",
      "            imputation_results.append(result_with_features)\n",
      "            print(f\"Accuracy with precomputed features: {result_with_features['accuracy']:.4f}\")\n",
      "            \n",
      "            # Si nous avons les deux résultats, calculer l'amélioration\n",
      "            standard_result = next((r for r in imputation_results if r['dataset'] == strategy), None)\n",
      "            if standard_result:\n",
      "                improvement = result_with_features['accuracy'] - standard_result['accuracy']\n",
      "                print(f\"Improvement from feature engineering: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
      "        else:\n",
      "            print(f\"Preprocessed dataset '{preprocessed_key}' not found in registry.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error processing preprocessed {strategy} dataset: {e}\")\n",
      "\n",
      "# Tester avec normalisation par ligne (optionnel)\n",
      "print(\"\\n--- Testing with row normalization ---\")\n",
      "for strategy in imputation_strategies:\n",
      "    preprocessed_key = f\"{strategy}_with_features\"\n",
      "    if preprocessed_key in DATASETS:\n",
      "        try:\n",
      "            print(f\"\\nTesting normalisation on {strategy} with precomputed features...\")\n",
      "            result_normalized = run_experiment(\n",
      "                dataset_key=preprocessed_key, \n",
      "                model_key=\"xgboost_baseline\", \n",
      "                add_feat=False,\n",
      "                normalize_by_row=True  # Activer la normalisation par ligne\n",
      "            )\n",
      "            results_tracker = add_result(results_tracker, result_normalized)\n",
      "            imputation_results.append(result_normalized)\n",
      "            print(f\"{strategy} with row normalization: Accuracy = {result_normalized['accuracy']:.4f}\")\n",
      "            \n",
      "            # Comparer avec version sans normalisation\n",
      "            non_normalized = next((r for r in imputation_results if r['dataset'] == preprocessed_key and not r.get('normalize_by_row', False)), None)\n",
      "            if non_normalized:\n",
      "                diff = result_normalized['accuracy'] - non_normalized['accuracy']\n",
      "                print(f\"Impact of row normalization: {diff:.4f} ({diff*100:.2f}%)\")\n",
      "        except Exception as e:\n",
      "            print(f\"Error with row normalization on {strategy}: {e}\")\n",
      "# --- Code Cell ---\n",
      "plt.figure(figsize=(12, 8))\n",
      "sns.barplot(x='dataset', y='accuracy', hue='features_added', \n",
      "            data=results_tracker[results_tracker['model'] == 'xgboost_baseline'])\n",
      "plt.title('Comparaison des stratégies d\\'imputation avec XGBoost')\n",
      "plt.xlabel('Stratégie d\\'imputation')\n",
      "plt.ylabel('Accuracy')\n",
      "plt.xticks(rotation=45)\n",
      "plt.legend(title='Features ajoutées')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Markdown Cell ---\n",
      "### 4.3 Régression logistique\n",
      "# \n",
      "# Un modèle linéaire simple pour établir une référence de base.\n",
      "\n",
      "# --- Code Cell ---\n",
      "for strategy in imputation_strategies:\n",
      "    try:\n",
      "        result = run_experiment(dataset_key=strategy, model_key=\"logistic\", add_feat=True)\n",
      "        results_tracker = add_result(results_tracker, result)\n",
      "        print(f\"Logistic regression on {strategy}: Accuracy = {result['accuracy']:.4f}\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error with logistic regression on {strategy}: {e}\")\n",
      "        \n",
      "# Comparer les performances des modèles sur les différentes stratégies d'imputation\n",
      "plt.figure(figsize=(14, 8))\n",
      "sns.barplot(x='dataset', y='accuracy', hue='model', \n",
      "            data=results_tracker[results_tracker['features_added'] == True])\n",
      "plt.title('Comparaison des modèles sur différentes stratégies d\\'imputation')\n",
      "plt.xlabel('Stratégie d\\'imputation')\n",
      "plt.ylabel('Accuracy')\n",
      "plt.xticks(rotation=45)\n",
      "plt.legend(title='Modèle')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### 4.4 Comparaison des benchmarks sur les différentes stratégies d'imputation\n",
      "# \n",
      "\n",
      "# --- Code Cell ---\n",
      "# 4.6 Analyse des résultats\n",
      "print(\"\\n--- Analyse des résultats ---\")\n",
      "\n",
      "# Trouver le meilleur résultat global\n",
      "best_result = results_tracker.loc[results_tracker['accuracy'].idxmax()]\n",
      "print(f\"\\nMeilleur résultat global:\")\n",
      "print(f\"Dataset: {best_result['dataset']} ({best_result['dataset_description']})\")\n",
      "print(f\"Model: {best_result['model']} ({best_result['model_description']})\")\n",
      "print(f\"Features Added: {best_result['features_added']}\")\n",
      "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
      "print(f\"Weighted F1-Score: {best_result['f1_weighted']:.4f}\")\n",
      "\n",
      "# Comparaison des stratégies d'imputation\n",
      "print(\"\\nComparaison des stratégies d'imputation (avec XGBoost):\")\n",
      "imputation_comparison = results_tracker[\n",
      "    (results_tracker['model'] == 'xgboost_baseline') & \n",
      "    (results_tracker['features_added'] == True)\n",
      "].sort_values('accuracy', ascending=False)\n",
      "\n",
      "for _, row in imputation_comparison.iterrows():\n",
      "    print(f\"{row['dataset']}: Accuracy = {row['accuracy']:.4f}, F1 = {row['f1_weighted']:.4f}\")\n",
      "\n",
      "# Performance vs Temps\n",
      "plt.figure(figsize=(12, 8))\n",
      "plt.scatter(results_tracker['total_time'], results_tracker['accuracy'], \n",
      "            c=results_tracker['dataset'].astype('category').cat.codes, \n",
      "            s=100, alpha=0.7)\n",
      "plt.xlabel('Temps total d\\'exécution (secondes)')\n",
      "plt.ylabel('Accuracy')\n",
      "plt.title('Performance vs. Temps d\\'exécution')\n",
      "plt.grid(True)\n",
      "plt.colorbar(label='Dataset')\n",
      "plt.tight_layout()\n",
      "plt.show()\n",
      "# --- Markdown Cell ---\n",
      "## 5. Analyse  des features\n",
      "# \n",
      "# Après avoir établi des benchmarks, nous cherchons à améliorer les performances en analysant et en créant de nouvelles features.\n",
      "# \n",
      "### 5.1 Analyse en composantes principales (PCA)\n",
      "# \n",
      "# La PCA nous aide à comprendre la structure des données et à réduire la dimensionnalité.\n",
      "\n",
      "# --- Code Cell ---\n",
      "# Effectuer l'analyse PCA sur le dataset ayant donné les meilleurs résultats\n",
      "# Selon les benchmarks, utiliser le dataset avec la meilleure performance\n",
      "best_dataset = \"ffbf\"  # À remplacer par le dataset ayant donné les meilleurs résultats\n",
      "pca_results = perform_pca_analysis(dataset_key=best_dataset, sample_size=50000)\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 5.2 Visualisation t-SNE\n",
      "\n",
      "# --- Code Cell ---\n",
      "# Effectuer l'analyse t-SNE sur le dataset ayant donné les meilleurs résultats\n",
      "tsne_results = perform_tsne_analysis(dataset_key=best_dataset, sample_size=5000)\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 5.3 Analyse des corrélations\n",
      "# \n",
      "# Identifions les features redondantes pour simplifier nos modèles.\n",
      "\n",
      "# --- Code Cell ---\n",
      "# Effectuer l'analyse des corrélations\n",
      "correlation_results = analyze_correlations(dataset_key=best_dataset)\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 5.4 Evaluation de différents ensembles de features. \n",
      "# Créons des features dérivées pour capturer des motifs complexes dans les rendements.\n",
      "\n",
      "# --- Code Cell ---\n",
      "\n",
      "# Évaluer les différents ensembles de features\n",
      "feature_evaluation = evaluate_feature_sets(dataset_key=best_dataset)\n",
      "# --- Markdown Cell ---\n",
      "### 5.5 Analyse des features les plus pertinentes\n",
      "# --- Code Cell ---\n",
      "# Analyser l'importance des features sur le meilleur dataset\n",
      "feature_importance, model = analyze_feature_importance(dataset_key=best_dataset, add_features=True, \n",
      "                                                     feature_sets=[\"basic_stats\", \"technical\"])\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### 5.6  Analyse des meilleures features\n",
      "# --- Code Cell ---\n",
      "# Sélectionner les meilleures features\n",
      "best_features = select_best_features(feature_importance, threshold=0.01)\n",
      "# --- Markdown Cell ---\n",
      "### Résumé \n",
      "# --- Code Cell ---\n",
      "summarize_feature_analysis(pca_results, correlation_results, feature_importance, best_features)\n",
      "# --- Markdown Cell ---\n",
      "## 6. Modèles optimisés\n",
      "# \n",
      "# Avec nos nouvelles insights et features, améliorons nos modèles.\n",
      "# \n",
      "### 6.1 Sélection des features optimales\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "# ```python\n",
      "# Code pour la sélection de features\n",
      "# ```\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### 6.2 Optimisation des hyperparamètres\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "# ```python\n",
      "# Code pour l'optimisation\n",
      "# ```\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 6.3 Comparaison avec les benchmarks\n",
      "# \n",
      "# ```python\n",
      "# Code pour la comparaison\n",
      "# ```\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "## 7. Méthodes avancées\n",
      "# \n",
      "# Explorons des approches plus sophistiquées pour améliorer encore les performances.\n",
      "# \n",
      "### 7.1 Clustering non supervisé\n",
      "\n",
      "# --- Code Cell ---\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "### 7.2 Modèles ensemblistes avancés\n",
      "# --- Code Cell ---\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "# ```python\n",
      "# Code pour les modèles ensemblistes\n",
      "# ```\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "## 8. Interprétabilité des modèles\n",
      "# \n",
      "# Analysons comment nos modèles prennent leurs décisions.\n",
      "# \n",
      "### 8.1 SHAP values\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "# ```python\n",
      "# Code pour SHAP\n",
      "# ```\n",
      "\n",
      "# --- Markdown Cell ---\n",
      "### 8.2 Analyse des erreurs\n",
      "# \n",
      "# \n",
      "\n",
      "# --- Markdown Cell ---\n",
      "# \n",
      "## 9. Conclusion\n",
      "# \n",
      "### 9.1 Synthèse des résultats\n",
      "# \n",
      "### 9.2 Recommandations\n",
      "# \n",
      "### 9.3 Perspectives futures\n",
      "# --- Markdown Cell ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_cells_from_notebook(notebook_path, include_code=True, include_markdown=True, output_path=None):\n",
    "    \"\"\"\n",
    "    Extracts code and/or markdown cells from a Jupyter notebook and returns them as a single text block.\n",
    "    \n",
    "    Parameters:\n",
    "        notebook_path (str): Path to the input .ipynb file\n",
    "        include_code (bool): Whether to include code cells\n",
    "        include_markdown (bool): Whether to include markdown cells\n",
    "        output_path (str or None): Path to save output (e.g., .py or .txt), or None to just return string\n",
    "\n",
    "    Returns:\n",
    "        str: Combined source content of selected cell types\n",
    "    \"\"\"\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    extracted_lines = []\n",
    "\n",
    "    for cell in notebook.get('cells', []):\n",
    "        cell_type = cell.get('cell_type')\n",
    "        if cell_type == 'code' and include_code:\n",
    "            extracted_lines.append(\"# --- Code Cell ---\\n\")\n",
    "            extracted_lines.extend(cell.get('source', []))\n",
    "            extracted_lines.append('\\n')\n",
    "        elif cell_type == 'markdown' and include_markdown:\n",
    "            extracted_lines.append(\"# --- Markdown Cell ---\\n\")\n",
    "            # Convert Markdown to commented lines (optional)\n",
    "            md_lines = cell.get('source', [])\n",
    "            commented_md = ['# ' + line if not line.startswith('#') else line for line in md_lines]\n",
    "            extracted_lines.extend(commented_md)\n",
    "            extracted_lines.append('\\n')\n",
    "\n",
    "    output = ''.join(extracted_lines)\n",
    "\n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "code_and_md = extract_cells_from_notebook(\n",
    "    notebook_path=\"redaction.ipynb\",\n",
    "    include_code=True,\n",
    "    include_markdown=True,\n",
    "    output_path=\"readction.txt\"\n",
    ")\n",
    "\n",
    "print(code_and_md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
